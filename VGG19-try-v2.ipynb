{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 20:55:19.408600: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-19 20:55:19.433625: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-19 20:55:19.466853: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-19 20:55:19.476340: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-19 20:55:19.500160: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-19 20:55:20.601747: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5195 images belonging to 4 classes.\n",
      "Found 1482 images belonging to 4 classes.\n",
      "Found 740 images belonging to 4 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 20:55:22.002080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22181 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:af:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/esidserver/miniforge3/envs/tf/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1726754129.454643 4007265 service.cc:146] XLA service 0x79216c003bd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1726754129.454675 4007265 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-09-19 20:55:29.496271: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-09-19 20:55:29.851936: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8902\n",
      "2024-09-19 20:55:30.675856: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_944', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2024-09-19 20:55:30.799568: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1343', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2024-09-19 20:55:31.172216: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1343', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2024-09-19 20:55:31.196114: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1343', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2024-09-19 20:55:31.239117: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1343', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2024-09-19 20:55:31.633080: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_944', 136 bytes spill stores, 136 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  1/163\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27:21\u001b[0m 10s/step - accuracy: 0.4375 - loss: 1.3178"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1726754134.825805 4007265 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 34/163\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 368ms/step - accuracy: 0.3142 - loss: 1.4029"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 20:55:49.197096: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_944', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m162/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 379ms/step - accuracy: 0.4243 - loss: 1.3129"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 20:56:43.981648: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_252', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 446ms/step - accuracy: 0.4254 - loss: 1.3118 - val_accuracy: 0.5607 - val_loss: 1.1022\n",
      "Epoch 2/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 376ms/step - accuracy: 0.6252 - loss: 1.0437 - val_accuracy: 0.6538 - val_loss: 0.9687\n",
      "Epoch 3/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 367ms/step - accuracy: 0.6830 - loss: 0.9182 - val_accuracy: 0.6775 - val_loss: 0.8888\n",
      "Epoch 4/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 373ms/step - accuracy: 0.7131 - loss: 0.8212 - val_accuracy: 0.6889 - val_loss: 0.8350\n",
      "Epoch 5/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 372ms/step - accuracy: 0.7346 - loss: 0.7542 - val_accuracy: 0.6957 - val_loss: 0.7955\n",
      "Epoch 6/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 364ms/step - accuracy: 0.7397 - loss: 0.7310 - val_accuracy: 0.7072 - val_loss: 0.7617\n",
      "Epoch 7/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 344ms/step - accuracy: 0.7640 - loss: 0.6629 - val_accuracy: 0.7099 - val_loss: 0.7519\n",
      "Epoch 8/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 345ms/step - accuracy: 0.7841 - loss: 0.6175 - val_accuracy: 0.7166 - val_loss: 0.7207\n",
      "Epoch 9/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 340ms/step - accuracy: 0.7895 - loss: 0.5870 - val_accuracy: 0.7078 - val_loss: 0.7224\n",
      "Epoch 10/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 341ms/step - accuracy: 0.8110 - loss: 0.5429 - val_accuracy: 0.7220 - val_loss: 0.6698\n",
      "Epoch 11/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 339ms/step - accuracy: 0.8039 - loss: 0.5406 - val_accuracy: 0.7126 - val_loss: 0.7186\n",
      "Epoch 12/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 339ms/step - accuracy: 0.8298 - loss: 0.5004 - val_accuracy: 0.6984 - val_loss: 0.7502\n",
      "Epoch 13/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 346ms/step - accuracy: 0.8135 - loss: 0.5089 - val_accuracy: 0.7220 - val_loss: 0.6512\n",
      "Epoch 14/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 338ms/step - accuracy: 0.8252 - loss: 0.4797 - val_accuracy: 0.7206 - val_loss: 0.6610\n",
      "Epoch 15/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 340ms/step - accuracy: 0.8263 - loss: 0.4781 - val_accuracy: 0.7328 - val_loss: 0.6460\n",
      "Epoch 16/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 341ms/step - accuracy: 0.8313 - loss: 0.4638 - val_accuracy: 0.7281 - val_loss: 0.6530\n",
      "Epoch 17/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 344ms/step - accuracy: 0.8314 - loss: 0.4571 - val_accuracy: 0.7301 - val_loss: 0.6370\n",
      "Epoch 18/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 344ms/step - accuracy: 0.8333 - loss: 0.4462 - val_accuracy: 0.7517 - val_loss: 0.6036\n",
      "Epoch 19/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 348ms/step - accuracy: 0.8471 - loss: 0.4306 - val_accuracy: 0.7159 - val_loss: 0.6685\n",
      "Epoch 20/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 340ms/step - accuracy: 0.8469 - loss: 0.4313 - val_accuracy: 0.7510 - val_loss: 0.5971\n",
      "Epoch 21/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 344ms/step - accuracy: 0.8544 - loss: 0.4102 - val_accuracy: 0.7578 - val_loss: 0.5904\n",
      "Epoch 22/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 342ms/step - accuracy: 0.8558 - loss: 0.4007 - val_accuracy: 0.7463 - val_loss: 0.6157\n",
      "Epoch 23/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 368ms/step - accuracy: 0.8485 - loss: 0.3924 - val_accuracy: 0.7605 - val_loss: 0.5925\n",
      "Epoch 24/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 372ms/step - accuracy: 0.8511 - loss: 0.3941 - val_accuracy: 0.7517 - val_loss: 0.5974\n",
      "Epoch 25/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 358ms/step - accuracy: 0.8564 - loss: 0.3852 - val_accuracy: 0.7449 - val_loss: 0.6120\n",
      "Epoch 26/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 374ms/step - accuracy: 0.8577 - loss: 0.3853 - val_accuracy: 0.7544 - val_loss: 0.6116\n",
      "Epoch 27/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 373ms/step - accuracy: 0.8542 - loss: 0.3791 - val_accuracy: 0.7645 - val_loss: 0.5762\n",
      "Epoch 28/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 340ms/step - accuracy: 0.8659 - loss: 0.3636 - val_accuracy: 0.7638 - val_loss: 0.5978\n",
      "Epoch 29/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 341ms/step - accuracy: 0.8695 - loss: 0.3550 - val_accuracy: 0.7537 - val_loss: 0.6122\n",
      "Epoch 30/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 343ms/step - accuracy: 0.8682 - loss: 0.3657 - val_accuracy: 0.7436 - val_loss: 0.6242\n",
      "Epoch 31/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 341ms/step - accuracy: 0.8728 - loss: 0.3493 - val_accuracy: 0.7409 - val_loss: 0.6473\n",
      "Epoch 32/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 340ms/step - accuracy: 0.8674 - loss: 0.3614 - val_accuracy: 0.7692 - val_loss: 0.5861\n",
      "Epoch 33/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 360ms/step - accuracy: 0.8657 - loss: 0.3556 - val_accuracy: 0.7827 - val_loss: 0.5627\n",
      "Epoch 34/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 345ms/step - accuracy: 0.8783 - loss: 0.3380 - val_accuracy: 0.7625 - val_loss: 0.6217\n",
      "Epoch 35/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 341ms/step - accuracy: 0.8752 - loss: 0.3440 - val_accuracy: 0.7470 - val_loss: 0.6263\n",
      "Epoch 36/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 342ms/step - accuracy: 0.8781 - loss: 0.3400 - val_accuracy: 0.7679 - val_loss: 0.5762\n",
      "Epoch 37/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 347ms/step - accuracy: 0.8797 - loss: 0.3272 - val_accuracy: 0.7632 - val_loss: 0.5919\n",
      "Epoch 38/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 360ms/step - accuracy: 0.8736 - loss: 0.3341 - val_accuracy: 0.7719 - val_loss: 0.5791\n",
      "Epoch 39/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 345ms/step - accuracy: 0.8790 - loss: 0.3308 - val_accuracy: 0.7814 - val_loss: 0.5610\n",
      "Epoch 40/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 338ms/step - accuracy: 0.8871 - loss: 0.3164 - val_accuracy: 0.7699 - val_loss: 0.5758\n",
      "Epoch 41/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 341ms/step - accuracy: 0.8809 - loss: 0.3211 - val_accuracy: 0.7665 - val_loss: 0.5969\n",
      "Epoch 42/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 350ms/step - accuracy: 0.8856 - loss: 0.3062 - val_accuracy: 0.7625 - val_loss: 0.6183\n",
      "Epoch 43/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 337ms/step - accuracy: 0.8877 - loss: 0.3011 - val_accuracy: 0.7618 - val_loss: 0.6028\n",
      "Epoch 44/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 345ms/step - accuracy: 0.8917 - loss: 0.3128 - val_accuracy: 0.7922 - val_loss: 0.5552\n",
      "Epoch 45/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 342ms/step - accuracy: 0.8917 - loss: 0.3009 - val_accuracy: 0.7874 - val_loss: 0.5612\n",
      "Epoch 46/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 341ms/step - accuracy: 0.8827 - loss: 0.3140 - val_accuracy: 0.7746 - val_loss: 0.5689\n",
      "Epoch 47/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 341ms/step - accuracy: 0.8789 - loss: 0.3231 - val_accuracy: 0.7746 - val_loss: 0.6187\n",
      "Epoch 48/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 346ms/step - accuracy: 0.8889 - loss: 0.3015 - val_accuracy: 0.7719 - val_loss: 0.5853\n",
      "Epoch 49/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 343ms/step - accuracy: 0.8901 - loss: 0.2917 - val_accuracy: 0.7773 - val_loss: 0.5643\n",
      "Epoch 50/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 342ms/step - accuracy: 0.8870 - loss: 0.2965 - val_accuracy: 0.7692 - val_loss: 0.5926\n",
      "Epoch 51/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 350ms/step - accuracy: 0.8955 - loss: 0.2896 - val_accuracy: 0.7928 - val_loss: 0.5518\n",
      "Epoch 52/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 347ms/step - accuracy: 0.8940 - loss: 0.2906 - val_accuracy: 0.7834 - val_loss: 0.5430\n",
      "Epoch 53/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 341ms/step - accuracy: 0.8956 - loss: 0.2938 - val_accuracy: 0.7814 - val_loss: 0.5671\n",
      "Epoch 54/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 337ms/step - accuracy: 0.8977 - loss: 0.2812 - val_accuracy: 0.7908 - val_loss: 0.5442\n",
      "Epoch 55/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 343ms/step - accuracy: 0.8882 - loss: 0.2957 - val_accuracy: 0.7719 - val_loss: 0.5844\n",
      "Epoch 56/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 340ms/step - accuracy: 0.8910 - loss: 0.2807 - val_accuracy: 0.7692 - val_loss: 0.5997\n",
      "Epoch 57/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 343ms/step - accuracy: 0.9004 - loss: 0.2760 - val_accuracy: 0.7821 - val_loss: 0.5541\n",
      "Epoch 58/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 347ms/step - accuracy: 0.8992 - loss: 0.2802 - val_accuracy: 0.7780 - val_loss: 0.5931\n",
      "Epoch 59/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 347ms/step - accuracy: 0.9043 - loss: 0.2651 - val_accuracy: 0.7861 - val_loss: 0.5562\n",
      "Epoch 60/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 343ms/step - accuracy: 0.8977 - loss: 0.2674 - val_accuracy: 0.7686 - val_loss: 0.6057\n",
      "Epoch 61/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 339ms/step - accuracy: 0.9006 - loss: 0.2764 - val_accuracy: 0.7686 - val_loss: 0.6061\n",
      "Epoch 62/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 355ms/step - accuracy: 0.9035 - loss: 0.2701 - val_accuracy: 0.7935 - val_loss: 0.5437\n",
      "Epoch 63/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 340ms/step - accuracy: 0.8998 - loss: 0.2607 - val_accuracy: 0.7794 - val_loss: 0.5709\n",
      "Epoch 64/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 348ms/step - accuracy: 0.8949 - loss: 0.2778 - val_accuracy: 0.7881 - val_loss: 0.5543\n",
      "Epoch 65/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 344ms/step - accuracy: 0.8975 - loss: 0.2712 - val_accuracy: 0.7888 - val_loss: 0.5550\n",
      "Epoch 66/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 346ms/step - accuracy: 0.9072 - loss: 0.2605 - val_accuracy: 0.7807 - val_loss: 0.5743\n",
      "Epoch 67/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 343ms/step - accuracy: 0.8995 - loss: 0.2693 - val_accuracy: 0.7753 - val_loss: 0.6021\n",
      "Epoch 68/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 346ms/step - accuracy: 0.9011 - loss: 0.2636 - val_accuracy: 0.7679 - val_loss: 0.6159\n",
      "Epoch 69/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 340ms/step - accuracy: 0.8995 - loss: 0.2716 - val_accuracy: 0.7881 - val_loss: 0.5550\n",
      "Epoch 70/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 351ms/step - accuracy: 0.8980 - loss: 0.2658 - val_accuracy: 0.7976 - val_loss: 0.5427\n",
      "Epoch 71/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 346ms/step - accuracy: 0.9031 - loss: 0.2583 - val_accuracy: 0.7827 - val_loss: 0.5636\n",
      "Epoch 72/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 344ms/step - accuracy: 0.9114 - loss: 0.2468 - val_accuracy: 0.7848 - val_loss: 0.5946\n",
      "Epoch 73/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 350ms/step - accuracy: 0.9003 - loss: 0.2573 - val_accuracy: 0.7854 - val_loss: 0.5510\n",
      "Epoch 74/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 346ms/step - accuracy: 0.9032 - loss: 0.2594 - val_accuracy: 0.7746 - val_loss: 0.5907\n",
      "Epoch 75/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 348ms/step - accuracy: 0.9021 - loss: 0.2508 - val_accuracy: 0.7773 - val_loss: 0.5805\n",
      "Epoch 76/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 338ms/step - accuracy: 0.9074 - loss: 0.2467 - val_accuracy: 0.7794 - val_loss: 0.5874\n",
      "Epoch 77/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 341ms/step - accuracy: 0.9080 - loss: 0.2452 - val_accuracy: 0.7976 - val_loss: 0.5525\n",
      "Epoch 78/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 346ms/step - accuracy: 0.9044 - loss: 0.2540 - val_accuracy: 0.7874 - val_loss: 0.5604\n",
      "Epoch 79/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 347ms/step - accuracy: 0.9107 - loss: 0.2449 - val_accuracy: 0.7740 - val_loss: 0.6065\n",
      "Epoch 80/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 349ms/step - accuracy: 0.9025 - loss: 0.2594 - val_accuracy: 0.7868 - val_loss: 0.5594\n",
      "Epoch 81/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 345ms/step - accuracy: 0.9093 - loss: 0.2496 - val_accuracy: 0.7834 - val_loss: 0.5680\n",
      "Epoch 82/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 349ms/step - accuracy: 0.9068 - loss: 0.2550 - val_accuracy: 0.7935 - val_loss: 0.5474\n",
      "Epoch 83/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 347ms/step - accuracy: 0.9084 - loss: 0.2536 - val_accuracy: 0.7935 - val_loss: 0.5705\n",
      "Epoch 84/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 349ms/step - accuracy: 0.9181 - loss: 0.2347 - val_accuracy: 0.7874 - val_loss: 0.5682\n",
      "Epoch 85/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 343ms/step - accuracy: 0.9183 - loss: 0.2281 - val_accuracy: 0.7854 - val_loss: 0.5737\n",
      "Epoch 86/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 342ms/step - accuracy: 0.9137 - loss: 0.2388 - val_accuracy: 0.7915 - val_loss: 0.5695\n",
      "Epoch 87/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 347ms/step - accuracy: 0.9055 - loss: 0.2501 - val_accuracy: 0.7955 - val_loss: 0.5549\n",
      "Epoch 88/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 343ms/step - accuracy: 0.9177 - loss: 0.2296 - val_accuracy: 0.7922 - val_loss: 0.5539\n",
      "Epoch 89/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 357ms/step - accuracy: 0.9075 - loss: 0.2408 - val_accuracy: 0.7942 - val_loss: 0.5504\n",
      "Epoch 90/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 348ms/step - accuracy: 0.9038 - loss: 0.2592 - val_accuracy: 0.7955 - val_loss: 0.5491\n",
      "Epoch 91/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 347ms/step - accuracy: 0.9116 - loss: 0.2296 - val_accuracy: 0.7935 - val_loss: 0.5542\n",
      "Epoch 92/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 346ms/step - accuracy: 0.9122 - loss: 0.2431 - val_accuracy: 0.7962 - val_loss: 0.5614\n",
      "Epoch 93/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 341ms/step - accuracy: 0.9219 - loss: 0.2235 - val_accuracy: 0.7969 - val_loss: 0.5457\n",
      "Epoch 94/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 349ms/step - accuracy: 0.9108 - loss: 0.2453 - val_accuracy: 0.7949 - val_loss: 0.5752\n",
      "Epoch 95/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 343ms/step - accuracy: 0.9134 - loss: 0.2320 - val_accuracy: 0.7928 - val_loss: 0.5695\n",
      "Epoch 96/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 345ms/step - accuracy: 0.9161 - loss: 0.2235 - val_accuracy: 0.7888 - val_loss: 0.5623\n",
      "Epoch 97/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 346ms/step - accuracy: 0.9216 - loss: 0.2171 - val_accuracy: 0.7868 - val_loss: 0.5766\n",
      "Epoch 98/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 345ms/step - accuracy: 0.9103 - loss: 0.2449 - val_accuracy: 0.7935 - val_loss: 0.5619\n",
      "Epoch 99/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 353ms/step - accuracy: 0.9211 - loss: 0.2215 - val_accuracy: 0.7976 - val_loss: 0.5482\n",
      "Epoch 100/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 347ms/step - accuracy: 0.9127 - loss: 0.2271 - val_accuracy: 0.7874 - val_loss: 0.5662\n",
      "Epoch 101/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 341ms/step - accuracy: 0.9252 - loss: 0.2118 - val_accuracy: 0.7962 - val_loss: 0.5525\n",
      "Epoch 102/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 342ms/step - accuracy: 0.9080 - loss: 0.2340 - val_accuracy: 0.7955 - val_loss: 0.5718\n",
      "Epoch 103/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 342ms/step - accuracy: 0.9103 - loss: 0.2399 - val_accuracy: 0.7848 - val_loss: 0.5873\n",
      "Epoch 104/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 342ms/step - accuracy: 0.9204 - loss: 0.2202 - val_accuracy: 0.7935 - val_loss: 0.5529\n",
      "Epoch 105/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 346ms/step - accuracy: 0.9095 - loss: 0.2398 - val_accuracy: 0.7834 - val_loss: 0.6265\n",
      "Epoch 106/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 342ms/step - accuracy: 0.9195 - loss: 0.2246 - val_accuracy: 0.8023 - val_loss: 0.5494\n",
      "Epoch 107/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 348ms/step - accuracy: 0.9270 - loss: 0.2151 - val_accuracy: 0.7942 - val_loss: 0.5797\n",
      "Epoch 108/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 341ms/step - accuracy: 0.9081 - loss: 0.2303 - val_accuracy: 0.7915 - val_loss: 0.5658\n",
      "Epoch 109/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 368ms/step - accuracy: 0.9202 - loss: 0.2187 - val_accuracy: 0.7935 - val_loss: 0.5576\n",
      "Epoch 110/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 344ms/step - accuracy: 0.9167 - loss: 0.2216 - val_accuracy: 0.7928 - val_loss: 0.5688\n",
      "Epoch 111/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 339ms/step - accuracy: 0.9185 - loss: 0.2186 - val_accuracy: 0.7982 - val_loss: 0.5472\n",
      "Epoch 112/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 359ms/step - accuracy: 0.9138 - loss: 0.2303 - val_accuracy: 0.7854 - val_loss: 0.6144\n",
      "Epoch 113/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 341ms/step - accuracy: 0.9219 - loss: 0.2143 - val_accuracy: 0.7901 - val_loss: 0.5747\n",
      "Epoch 114/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 343ms/step - accuracy: 0.9150 - loss: 0.2244 - val_accuracy: 0.7989 - val_loss: 0.5581\n",
      "Epoch 115/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 341ms/step - accuracy: 0.9123 - loss: 0.2238 - val_accuracy: 0.7962 - val_loss: 0.5541\n",
      "Epoch 116/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 352ms/step - accuracy: 0.9170 - loss: 0.2275 - val_accuracy: 0.7874 - val_loss: 0.6028\n",
      "Epoch 117/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 348ms/step - accuracy: 0.9202 - loss: 0.2300 - val_accuracy: 0.7908 - val_loss: 0.5750\n",
      "Epoch 118/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 343ms/step - accuracy: 0.9154 - loss: 0.2234 - val_accuracy: 0.7955 - val_loss: 0.5589\n",
      "Epoch 119/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 349ms/step - accuracy: 0.9193 - loss: 0.2145 - val_accuracy: 0.7915 - val_loss: 0.5910\n",
      "Epoch 120/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 346ms/step - accuracy: 0.9212 - loss: 0.2176 - val_accuracy: 0.8009 - val_loss: 0.5537\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Set paths to your dataset folders\n",
    "train_dir = '/home/esidserver/datasets/testing_cls/training-v3/train/'\n",
    "val_dir = '/home/esidserver/datasets/testing_cls/training-v3/val/'\n",
    "test_dir = '/home/esidserver/datasets/testing_cls/training-v3/test/'\n",
    "\n",
    "# Image data generator with data augmentation for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Data generator for validation and test\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Data loaders\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_test_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = val_test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Load the VGG19 model with pre-trained weights\n",
    "base_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Add custom top layers for classification\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze the base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping and model checkpoint\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('vgg19_best_model.keras', save_best_only=True, monitor='val_loss')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=250,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 22:54:06.247073: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1654', 76 bytes spill stores, 76 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 421ms/step - accuracy: 0.8962 - loss: 0.2887 - val_accuracy: 0.8084 - val_loss: 0.5737\n",
      "Epoch 2/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 351ms/step - accuracy: 0.9446 - loss: 0.1432 - val_accuracy: 0.8381 - val_loss: 0.4739\n",
      "Epoch 3/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 342ms/step - accuracy: 0.9582 - loss: 0.1250 - val_accuracy: 0.8394 - val_loss: 0.5110\n",
      "Epoch 4/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 351ms/step - accuracy: 0.9742 - loss: 0.0719 - val_accuracy: 0.8650 - val_loss: 0.4141\n",
      "Epoch 5/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 345ms/step - accuracy: 0.9792 - loss: 0.0561 - val_accuracy: 0.8657 - val_loss: 0.4787\n",
      "Epoch 6/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 347ms/step - accuracy: 0.9808 - loss: 0.0484 - val_accuracy: 0.8866 - val_loss: 0.4033\n",
      "Epoch 7/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 342ms/step - accuracy: 0.9887 - loss: 0.0350 - val_accuracy: 0.8354 - val_loss: 0.6180\n",
      "Epoch 8/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 338ms/step - accuracy: 0.9864 - loss: 0.0421 - val_accuracy: 0.8596 - val_loss: 0.5379\n",
      "Epoch 9/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 340ms/step - accuracy: 0.9892 - loss: 0.0307 - val_accuracy: 0.8509 - val_loss: 0.5028\n",
      "Epoch 10/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 340ms/step - accuracy: 0.9906 - loss: 0.0280 - val_accuracy: 0.8765 - val_loss: 0.4217\n",
      "Epoch 11/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 341ms/step - accuracy: 0.9872 - loss: 0.0361 - val_accuracy: 0.8738 - val_loss: 0.4107\n",
      "Epoch 12/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 340ms/step - accuracy: 0.9945 - loss: 0.0180 - val_accuracy: 0.8981 - val_loss: 0.4088\n",
      "Epoch 13/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 340ms/step - accuracy: 0.9892 - loss: 0.0346 - val_accuracy: 0.9062 - val_loss: 0.4343\n",
      "Epoch 14/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 339ms/step - accuracy: 0.9940 - loss: 0.0257 - val_accuracy: 0.8853 - val_loss: 0.4609\n",
      "Epoch 15/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 343ms/step - accuracy: 0.9960 - loss: 0.0105 - val_accuracy: 0.8968 - val_loss: 0.4030\n",
      "Epoch 16/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 344ms/step - accuracy: 0.9954 - loss: 0.0139 - val_accuracy: 0.9035 - val_loss: 0.3919\n",
      "Epoch 17/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 339ms/step - accuracy: 0.9944 - loss: 0.0146 - val_accuracy: 0.8468 - val_loss: 0.5843\n",
      "Epoch 18/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 337ms/step - accuracy: 0.9925 - loss: 0.0204 - val_accuracy: 0.8873 - val_loss: 0.4294\n",
      "Epoch 19/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 343ms/step - accuracy: 0.9946 - loss: 0.0172 - val_accuracy: 0.9055 - val_loss: 0.4160\n",
      "Epoch 20/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 343ms/step - accuracy: 0.9938 - loss: 0.0171 - val_accuracy: 0.8677 - val_loss: 0.6331\n",
      "Epoch 21/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 339ms/step - accuracy: 0.9886 - loss: 0.0385 - val_accuracy: 0.8995 - val_loss: 0.3936\n",
      "Epoch 22/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 344ms/step - accuracy: 0.9933 - loss: 0.0185 - val_accuracy: 0.8738 - val_loss: 0.5175\n",
      "Epoch 23/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 343ms/step - accuracy: 0.9926 - loss: 0.0193 - val_accuracy: 0.8725 - val_loss: 0.5574\n",
      "Epoch 24/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 349ms/step - accuracy: 0.9972 - loss: 0.0114 - val_accuracy: 0.8590 - val_loss: 0.6685\n",
      "Epoch 25/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 341ms/step - accuracy: 0.9933 - loss: 0.0214 - val_accuracy: 0.8819 - val_loss: 0.5409\n",
      "Epoch 26/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 352ms/step - accuracy: 0.9910 - loss: 0.0257 - val_accuracy: 0.9008 - val_loss: 0.3754\n",
      "Epoch 27/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 342ms/step - accuracy: 0.9981 - loss: 0.0079 - val_accuracy: 0.8941 - val_loss: 0.3982\n",
      "Epoch 28/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 341ms/step - accuracy: 0.9965 - loss: 0.0122 - val_accuracy: 0.8596 - val_loss: 0.7408\n",
      "Epoch 29/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 341ms/step - accuracy: 0.9949 - loss: 0.0153 - val_accuracy: 0.8974 - val_loss: 0.4437\n",
      "Epoch 30/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 338ms/step - accuracy: 0.9964 - loss: 0.0110 - val_accuracy: 0.8981 - val_loss: 0.3935\n",
      "Epoch 31/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 337ms/step - accuracy: 0.9900 - loss: 0.0317 - val_accuracy: 0.8934 - val_loss: 0.3935\n",
      "Epoch 32/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 340ms/step - accuracy: 0.9963 - loss: 0.0106 - val_accuracy: 0.9008 - val_loss: 0.3837\n",
      "Epoch 33/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 346ms/step - accuracy: 0.9974 - loss: 0.0061 - val_accuracy: 0.9103 - val_loss: 0.3407\n",
      "Epoch 34/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 343ms/step - accuracy: 0.9920 - loss: 0.0231 - val_accuracy: 0.8691 - val_loss: 0.5208\n",
      "Epoch 35/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 350ms/step - accuracy: 0.9953 - loss: 0.0133 - val_accuracy: 0.9103 - val_loss: 0.3061\n",
      "Epoch 36/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 342ms/step - accuracy: 0.9927 - loss: 0.0194 - val_accuracy: 0.8704 - val_loss: 0.4592\n",
      "Epoch 37/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 347ms/step - accuracy: 0.9964 - loss: 0.0101 - val_accuracy: 0.8974 - val_loss: 0.3948\n",
      "Epoch 38/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 346ms/step - accuracy: 0.9980 - loss: 0.0057 - val_accuracy: 0.8684 - val_loss: 0.5990\n",
      "Epoch 39/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 346ms/step - accuracy: 0.9931 - loss: 0.0266 - val_accuracy: 0.8941 - val_loss: 0.4923\n",
      "Epoch 40/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 348ms/step - accuracy: 0.9980 - loss: 0.0067 - val_accuracy: 0.9143 - val_loss: 0.3745\n",
      "Epoch 41/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 342ms/step - accuracy: 0.9984 - loss: 0.0045 - val_accuracy: 0.9076 - val_loss: 0.3829\n",
      "Epoch 42/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 340ms/step - accuracy: 0.9984 - loss: 0.0061 - val_accuracy: 0.8900 - val_loss: 0.4820\n",
      "Epoch 43/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 343ms/step - accuracy: 0.9964 - loss: 0.0118 - val_accuracy: 0.8860 - val_loss: 0.4516\n",
      "Epoch 44/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 339ms/step - accuracy: 0.9938 - loss: 0.0178 - val_accuracy: 0.8900 - val_loss: 0.4413\n",
      "Epoch 45/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 343ms/step - accuracy: 0.9967 - loss: 0.0091 - val_accuracy: 0.8968 - val_loss: 0.4681\n",
      "Epoch 46/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 341ms/step - accuracy: 0.9970 - loss: 0.0083 - val_accuracy: 0.9015 - val_loss: 0.4338\n",
      "Epoch 47/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 343ms/step - accuracy: 0.9984 - loss: 0.0046 - val_accuracy: 0.9008 - val_loss: 0.4626\n",
      "Epoch 48/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 346ms/step - accuracy: 0.9936 - loss: 0.0224 - val_accuracy: 0.9291 - val_loss: 0.2584\n",
      "Epoch 49/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 340ms/step - accuracy: 0.9962 - loss: 0.0126 - val_accuracy: 0.8819 - val_loss: 0.5527\n",
      "Epoch 50/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 341ms/step - accuracy: 0.9989 - loss: 0.0048 - val_accuracy: 0.9008 - val_loss: 0.4255\n",
      "Epoch 51/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 339ms/step - accuracy: 0.9973 - loss: 0.0064 - val_accuracy: 0.9170 - val_loss: 0.3717\n",
      "Epoch 52/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 340ms/step - accuracy: 0.9960 - loss: 0.0086 - val_accuracy: 0.9231 - val_loss: 0.3341\n",
      "Epoch 53/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 346ms/step - accuracy: 0.9946 - loss: 0.0165 - val_accuracy: 0.9136 - val_loss: 0.3453\n",
      "Epoch 54/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 341ms/step - accuracy: 0.9925 - loss: 0.0269 - val_accuracy: 0.9103 - val_loss: 0.3380\n",
      "Epoch 55/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 338ms/step - accuracy: 0.9965 - loss: 0.0092 - val_accuracy: 0.9244 - val_loss: 0.3712\n",
      "Epoch 56/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 339ms/step - accuracy: 0.9983 - loss: 0.0044 - val_accuracy: 0.9143 - val_loss: 0.3976\n",
      "Epoch 57/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 339ms/step - accuracy: 0.9991 - loss: 0.0039 - val_accuracy: 0.9217 - val_loss: 0.3865\n",
      "Epoch 58/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 341ms/step - accuracy: 0.9950 - loss: 0.0175 - val_accuracy: 0.9109 - val_loss: 0.3647\n",
      "Epoch 59/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 339ms/step - accuracy: 0.9975 - loss: 0.0067 - val_accuracy: 0.8853 - val_loss: 0.6139\n",
      "Epoch 60/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 349ms/step - accuracy: 0.9945 - loss: 0.0156 - val_accuracy: 0.8907 - val_loss: 0.4334\n",
      "Epoch 61/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 338ms/step - accuracy: 0.9976 - loss: 0.0066 - val_accuracy: 0.9055 - val_loss: 0.4105\n",
      "Epoch 62/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 343ms/step - accuracy: 0.9998 - loss: 0.0016 - val_accuracy: 0.9130 - val_loss: 0.3701\n",
      "Epoch 63/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 339ms/step - accuracy: 0.9979 - loss: 0.0090 - val_accuracy: 0.9035 - val_loss: 0.3959\n",
      "Epoch 64/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 346ms/step - accuracy: 0.9985 - loss: 0.0069 - val_accuracy: 0.8860 - val_loss: 0.4347\n",
      "Epoch 65/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 340ms/step - accuracy: 0.9968 - loss: 0.0085 - val_accuracy: 0.8995 - val_loss: 0.5114\n",
      "Epoch 66/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 340ms/step - accuracy: 0.9959 - loss: 0.0091 - val_accuracy: 0.8907 - val_loss: 0.4139\n",
      "Epoch 67/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 342ms/step - accuracy: 0.9987 - loss: 0.0043 - val_accuracy: 0.9130 - val_loss: 0.3623\n",
      "Epoch 68/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 340ms/step - accuracy: 0.9984 - loss: 0.0046 - val_accuracy: 0.9224 - val_loss: 0.2838\n",
      "Epoch 69/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 342ms/step - accuracy: 0.9982 - loss: 0.0042 - val_accuracy: 0.9211 - val_loss: 0.3117\n",
      "Epoch 70/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 349ms/step - accuracy: 0.9980 - loss: 0.0068 - val_accuracy: 0.8947 - val_loss: 0.4472\n",
      "Epoch 71/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 341ms/step - accuracy: 0.9942 - loss: 0.0144 - val_accuracy: 0.8765 - val_loss: 0.5514\n",
      "Epoch 72/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 338ms/step - accuracy: 0.9949 - loss: 0.0096 - val_accuracy: 0.9062 - val_loss: 0.4174\n",
      "Epoch 73/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 341ms/step - accuracy: 0.9981 - loss: 0.0048 - val_accuracy: 0.9035 - val_loss: 0.5079\n",
      "Epoch 74/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 341ms/step - accuracy: 0.9961 - loss: 0.0126 - val_accuracy: 0.8947 - val_loss: 0.4198\n",
      "Epoch 75/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 342ms/step - accuracy: 0.9994 - loss: 0.0026 - val_accuracy: 0.8900 - val_loss: 0.4852\n",
      "Epoch 76/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 340ms/step - accuracy: 0.9961 - loss: 0.0101 - val_accuracy: 0.8664 - val_loss: 0.7456\n",
      "Epoch 77/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 340ms/step - accuracy: 0.9960 - loss: 0.0124 - val_accuracy: 0.9028 - val_loss: 0.3810\n",
      "Epoch 78/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 340ms/step - accuracy: 0.9990 - loss: 0.0043 - val_accuracy: 0.9015 - val_loss: 0.4215\n",
      "Epoch 79/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 350ms/step - accuracy: 0.9974 - loss: 0.0061 - val_accuracy: 0.9184 - val_loss: 0.3133\n",
      "Epoch 80/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 338ms/step - accuracy: 0.9996 - loss: 0.0020 - val_accuracy: 0.9076 - val_loss: 0.3449\n",
      "Epoch 81/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 340ms/step - accuracy: 0.9974 - loss: 0.0071 - val_accuracy: 0.8900 - val_loss: 0.4496\n",
      "Epoch 82/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 349ms/step - accuracy: 0.9959 - loss: 0.0155 - val_accuracy: 0.9170 - val_loss: 0.3749\n",
      "Epoch 83/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 338ms/step - accuracy: 0.9967 - loss: 0.0078 - val_accuracy: 0.9244 - val_loss: 0.3274\n",
      "Epoch 84/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 338ms/step - accuracy: 0.9976 - loss: 0.0096 - val_accuracy: 0.9204 - val_loss: 0.3517\n",
      "Epoch 85/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 343ms/step - accuracy: 0.9978 - loss: 0.0063 - val_accuracy: 0.9197 - val_loss: 0.3436\n",
      "Epoch 86/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 345ms/step - accuracy: 0.9967 - loss: 0.0106 - val_accuracy: 0.9271 - val_loss: 0.3724\n",
      "Epoch 87/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 340ms/step - accuracy: 0.9956 - loss: 0.0129 - val_accuracy: 0.9271 - val_loss: 0.2931\n",
      "Epoch 88/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 346ms/step - accuracy: 0.9972 - loss: 0.0057 - val_accuracy: 0.9305 - val_loss: 0.2944\n",
      "Epoch 89/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 339ms/step - accuracy: 0.9990 - loss: 0.0029 - val_accuracy: 0.9244 - val_loss: 0.3458\n",
      "Epoch 90/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 340ms/step - accuracy: 0.9976 - loss: 0.0069 - val_accuracy: 0.9130 - val_loss: 0.3466\n",
      "Epoch 91/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 347ms/step - accuracy: 0.9979 - loss: 0.0057 - val_accuracy: 0.9339 - val_loss: 0.2978\n",
      "Epoch 92/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 339ms/step - accuracy: 0.9993 - loss: 0.0023 - val_accuracy: 0.9008 - val_loss: 0.5032\n",
      "Epoch 93/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 344ms/step - accuracy: 0.9994 - loss: 0.0017 - val_accuracy: 0.9163 - val_loss: 0.4458\n",
      "Epoch 94/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 337ms/step - accuracy: 0.9970 - loss: 0.0094 - val_accuracy: 0.9184 - val_loss: 0.3519\n",
      "Epoch 95/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 343ms/step - accuracy: 0.9980 - loss: 0.0076 - val_accuracy: 0.9096 - val_loss: 0.3867\n",
      "Epoch 96/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 342ms/step - accuracy: 0.9980 - loss: 0.0050 - val_accuracy: 0.9285 - val_loss: 0.3132\n",
      "Epoch 97/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 342ms/step - accuracy: 0.9962 - loss: 0.0116 - val_accuracy: 0.9177 - val_loss: 0.2996\n",
      "Epoch 98/100\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 339ms/step - accuracy: 0.9984 - loss: 0.0043 - val_accuracy: 0.9339 - val_loss: 0.2722\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning: Unfreeze some layers of the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Compile the model again\n",
    "model.compile(optimizer=Adam(learning_rate=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Continue training with fine-tuning for 100 epochs\n",
    "history_finetune = model.fit(\n",
    "    train_generator,\n",
    "    epochs=100,  # Fine-tuning for 100 epochs\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.9152 - loss: 0.3762"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-20 00:29:56.370219: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_252', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 210ms/step - accuracy: 0.9152 - loss: 0.3760\n",
      "Final Training Accuracy: 0.9988\n",
      "Final Validation Accuracy: 0.9339\n",
      "Test Accuracy: 0.9149, Test Loss: 0.3730\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test set\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "\n",
    "# Calculate final training and validation accuracy\n",
    "train_acc = history_finetune.history['accuracy'][-1] if 'accuracy' in history_finetune.history else None\n",
    "val_acc = history_finetune.history['val_accuracy'][-1] if 'val_accuracy' in history_finetune.history else None\n",
    "\n",
    "# Print the results\n",
    "print(f\"Final Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Save the final model\n",
    "model.save('vgg19_final_model.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
