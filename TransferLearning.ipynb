{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed for our Cats & Dogs classes\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "# Fixed for Cats & Dogs color images\n",
    "CHANNELS = 3\n",
    "\n",
    "IMAGE_RESIZE = 224\n",
    "RESNET50_POOLING_AVERAGE = 'avg'\n",
    "DENSE_LAYER_ACTIVATION = 'softmax'\n",
    "OBJECTIVE_FUNCTION = 'categorical_crossentropy'\n",
    "\n",
    "# Common accuracy metric for all outputs, but can use different metrics for different output\n",
    "LOSS_METRICS = ['accuracy']\n",
    "\n",
    "# EARLY_STOP_PATIENCE must be < NUM_EPOCHS\n",
    "NUM_EPOCHS = 10\n",
    "EARLY_STOP_PATIENCE = 3\n",
    "\n",
    "# These steps value should be proper FACTOR of no.-of-images in train & valid folders respectively\n",
    "# Training images processed in each step would be no.-of-train-images / STEPS_PER_EPOCH_TRAINING\n",
    "STEPS_PER_EPOCH_TRAINING = 10\n",
    "STEPS_PER_EPOCH_VALIDATION = 10\n",
    "\n",
    "# These steps value should be proper FACTOR of no.-of-images in train & valid folders respectively\n",
    "# NOTE that these BATCH* are for Keras ImageDataGenerator batching to fill epoch step input\n",
    "BATCH_SIZE_TRAINING = 100\n",
    "BATCH_SIZE_VALIDATION = 100\n",
    "\n",
    "# Using 1 to easily manage mapping between test_generator & prediction for submission preparation\n",
    "BATCH_SIZE_TESTING = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_weights_path = 'resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resnet_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Still not talking about our train/test data or any pre-processing.\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 1st layer as the lumpsum weights from resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
    "# NOTE that this layer will be set below as NOT TRAINABLE, i.e., use it as is\n",
    "model.add(ResNet50(include_top = False, pooling = RESNET50_POOLING_AVERAGE, weights = resnet_weights_path))\n",
    "\n",
    "# 2nd layer as Dense for 2-class classification, i.e., dog or cat using SoftMax activation\n",
    "model.add(Dense(NUM_CLASSES, activation = DENSE_LAYER_ACTIVATION))\n",
    "\n",
    "# Say not to train first layer (ResNet) model as it is already trained\n",
    "model.layers[0].trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "predictions = Dense(4, activation='softmax')(x)  # Assuming 10 classes\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "sgd = optimizers.SGD(learning_rate=0.01, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "model.compile(optimizer = sgd, loss = OBJECTIVE_FUNCTION, metrics = LOSS_METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "image_size = IMAGE_RESIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=r\"./train/\",\n",
    "    target_size=(image_size, image_size),\n",
    "    color_mode=\"rgb\",\n",
    "    \n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True,\n",
    "    \n",
    "    batch_size=BATCH_SIZE_VALIDATION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_generator = train_datagen.flow_from_directory(\n",
    "    directory=r\"./valid/\",\n",
    "    target_size=(image_size, image_size),\n",
    "    color_mode=\"rgb\",\n",
    "    \n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True,\n",
    "    \n",
    "    batch_size=BATCH_SIZE_VALIDATION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(BATCH_SIZE_TRAINING, len(train_generator), BATCH_SIZE_VALIDATION, len(valid_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "cb_early_stopper = EarlyStopping(monitor = 'val_loss', patience = EARLY_STOP_PATIENCE)\n",
    "cb_checkpointer = ModelCheckpoint(filepath = '../working/best.keras', monitor = 'val_loss', save_best_only = True, mode = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_history = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=STEPS_PER_EPOCH_TRAINING,\n",
    "        epochs = NUM_EPOCHS,\n",
    "        validation_data=valid_generator,\n",
    "        validation_steps=STEPS_PER_EPOCH_VALIDATION,\n",
    "        callbacks=[cb_checkpointer, cb_early_stopper]\n",
    ")\n",
    "model.save_weights(\"../best.hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(fit_history.history.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    " plt.figure(1, figsize = (15,8)) \n",
    "    \n",
    "plt.subplot(221)  \n",
    "plt.plot(fit_history.history['acc'])  \n",
    "plt.plot(fit_history.history['val_acc'])  \n",
    "plt.title('model accuracy')  \n",
    "plt.ylabel('accuracy')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'valid']) \n",
    "    \n",
    "plt.subplot(222)  \n",
    "plt.plot(fit_history.history['loss'])  \n",
    "plt.plot(fit_history.history['val_loss'])  \n",
    "plt.title('model loss')  \n",
    "plt.ylabel('loss')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'valid']) \n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_generator = train_datagen.flow_from_directory(\n",
    "    directory=r\"./test/\",\n",
    "    target_size=(image_size, image_size),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=BATCH_SIZE_TESTING,\n",
    "    class_mode=None,\n",
    "    shuffle=False,\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "# ??train_datagen.flow_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator.reset()\n",
    "\n",
    "pred = model.predict_generator(test_generator, steps = len(test_generator), verbose = 1)\n",
    "\n",
    "predicted_class_indices = np.argmax(pred, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DIR = 'test/'\n",
    "f, ax = plt.subplots(5, 5, figsize = (15, 15))\n",
    "\n",
    "for i in range(0,25):\n",
    "    imgBGR = cv2.imread(TEST_DIR + test_generator.filenames[i])\n",
    "    imgRGB = cv2.cvtColor(imgBGR, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # a if condition else b\n",
    "    predicted_class = \"Dog\" if predicted_class_indices[i] else \"Cat\"\n",
    "\n",
    "    ax[i//5, i%5].imshow(imgRGB)\n",
    "    ax[i//5, i%5].axis('off')\n",
    "    ax[i//5, i%5].set_title(\"Predicted:{}\".format(predicted_class))    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5193 images belonging to 4 classes.\n",
      "Found 1484 images belonging to 4 classes.\n",
      "Found 740 images belonging to 4 classes.\n",
      "Epoch 1/250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/esidserver/miniforge3/envs/tf/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1724847912.803130  335646 service.cc:146] XLA service 0x7b086c0026c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1724847912.803209  335646 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-08-28 19:25:13.133559: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-08-28 19:25:14.421861: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8902\n",
      "2024-08-28 19:25:15.004123: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_5694', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2024-08-28 19:25:15.663138: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_5887', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2024-08-28 19:25:16.016023: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_5887', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2024-08-28 19:25:16.074983: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_5694', 136 bytes spill stores, 136 bytes spill loads\n",
      "\n",
      "2024-08-28 19:25:16.237615: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_5680', 176 bytes spill stores, 468 bytes spill loads\n",
      "\n",
      "2024-08-28 19:25:16.255213: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_5887', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2024-08-28 19:25:16.310587: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_5887', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  1/163\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m47:45\u001b[0m 18s/step - accuracy: 0.2188 - loss: 1.5142"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1724847920.203499  335646 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m107/163\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 436ms/step - accuracy: 0.2699 - loss: 1.4009"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-28 19:26:09.442768: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_5694', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m162/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 450ms/step - accuracy: 0.2738 - loss: 1.3932"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-28 19:26:47.394587: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1719', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 565ms/step - accuracy: 0.2740 - loss: 1.3930 - val_accuracy: 0.3706 - val_loss: 1.3356\n",
      "Epoch 2/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 381ms/step - accuracy: 0.3807 - loss: 1.3278 - val_accuracy: 0.4420 - val_loss: 1.2673\n",
      "Epoch 3/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 384ms/step - accuracy: 0.3927 - loss: 1.2872 - val_accuracy: 0.3026 - val_loss: 1.2880\n",
      "Epoch 4/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 402ms/step - accuracy: 0.4139 - loss: 1.2618 - val_accuracy: 0.4515 - val_loss: 1.2254\n",
      "Epoch 5/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 391ms/step - accuracy: 0.4354 - loss: 1.2407 - val_accuracy: 0.4832 - val_loss: 1.1951\n",
      "Epoch 6/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 390ms/step - accuracy: 0.4413 - loss: 1.2164 - val_accuracy: 0.5398 - val_loss: 1.1573\n",
      "Epoch 7/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 401ms/step - accuracy: 0.4587 - loss: 1.1948 - val_accuracy: 0.4939 - val_loss: 1.1352\n",
      "Epoch 8/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 398ms/step - accuracy: 0.4511 - loss: 1.1868 - val_accuracy: 0.5236 - val_loss: 1.1240\n",
      "Epoch 9/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 426ms/step - accuracy: 0.4655 - loss: 1.1644 - val_accuracy: 0.5593 - val_loss: 1.1170\n",
      "Epoch 10/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 388ms/step - accuracy: 0.4660 - loss: 1.1636 - val_accuracy: 0.5135 - val_loss: 1.1077\n",
      "Epoch 11/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 405ms/step - accuracy: 0.4738 - loss: 1.1548 - val_accuracy: 0.5047 - val_loss: 1.0962\n",
      "Epoch 12/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 389ms/step - accuracy: 0.4613 - loss: 1.1571 - val_accuracy: 0.5243 - val_loss: 1.0900\n",
      "Epoch 13/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 400ms/step - accuracy: 0.4841 - loss: 1.1472 - val_accuracy: 0.5323 - val_loss: 1.0690\n",
      "Epoch 14/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 387ms/step - accuracy: 0.4749 - loss: 1.1385 - val_accuracy: 0.5357 - val_loss: 1.0680\n",
      "Epoch 15/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 397ms/step - accuracy: 0.4908 - loss: 1.1267 - val_accuracy: 0.5687 - val_loss: 1.0654\n",
      "Epoch 16/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 379ms/step - accuracy: 0.4866 - loss: 1.1259 - val_accuracy: 0.5168 - val_loss: 1.0728\n",
      "Epoch 17/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 375ms/step - accuracy: 0.4870 - loss: 1.1142 - val_accuracy: 0.5451 - val_loss: 1.0632\n",
      "Epoch 18/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 382ms/step - accuracy: 0.4870 - loss: 1.1103 - val_accuracy: 0.5721 - val_loss: 1.0351\n",
      "Epoch 19/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 355ms/step - accuracy: 0.4940 - loss: 1.1141 - val_accuracy: 0.5296 - val_loss: 1.0682\n",
      "Epoch 20/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 384ms/step - accuracy: 0.4990 - loss: 1.1184 - val_accuracy: 0.5721 - val_loss: 1.0302\n",
      "Epoch 21/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 375ms/step - accuracy: 0.4962 - loss: 1.1114 - val_accuracy: 0.5532 - val_loss: 1.0394\n",
      "Epoch 22/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 380ms/step - accuracy: 0.5077 - loss: 1.0922 - val_accuracy: 0.5633 - val_loss: 1.0153\n",
      "Epoch 23/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 367ms/step - accuracy: 0.5120 - loss: 1.0868 - val_accuracy: 0.5620 - val_loss: 1.0157\n",
      "Epoch 24/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 388ms/step - accuracy: 0.5059 - loss: 1.1007 - val_accuracy: 0.5633 - val_loss: 1.0130\n",
      "Epoch 25/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 362ms/step - accuracy: 0.5045 - loss: 1.0994 - val_accuracy: 0.5916 - val_loss: 1.0184\n",
      "Epoch 26/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 387ms/step - accuracy: 0.5112 - loss: 1.0765 - val_accuracy: 0.5633 - val_loss: 1.0124\n",
      "Epoch 27/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 387ms/step - accuracy: 0.4961 - loss: 1.0897 - val_accuracy: 0.5606 - val_loss: 0.9981\n",
      "Epoch 28/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 366ms/step - accuracy: 0.5235 - loss: 1.0793 - val_accuracy: 0.5411 - val_loss: 1.0053\n",
      "Epoch 29/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 371ms/step - accuracy: 0.5191 - loss: 1.0706 - val_accuracy: 0.5391 - val_loss: 1.0315\n",
      "Epoch 30/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 370ms/step - accuracy: 0.5056 - loss: 1.0800 - val_accuracy: 0.5761 - val_loss: 1.0110\n",
      "Epoch 31/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 383ms/step - accuracy: 0.5110 - loss: 1.0810 - val_accuracy: 0.5735 - val_loss: 1.0068\n",
      "Epoch 32/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 369ms/step - accuracy: 0.5023 - loss: 1.0956 - val_accuracy: 0.5499 - val_loss: 1.0112\n",
      "Epoch 33/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 366ms/step - accuracy: 0.5126 - loss: 1.0779 - val_accuracy: 0.5451 - val_loss: 1.0148\n",
      "Epoch 34/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 377ms/step - accuracy: 0.5106 - loss: 1.0851 - val_accuracy: 0.5788 - val_loss: 1.0269\n",
      "Epoch 35/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 358ms/step - accuracy: 0.5075 - loss: 1.0668 - val_accuracy: 0.5923 - val_loss: 1.0435\n",
      "Epoch 36/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 358ms/step - accuracy: 0.5184 - loss: 1.0816 - val_accuracy: 0.5398 - val_loss: 1.0224\n",
      "Epoch 37/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 381ms/step - accuracy: 0.5135 - loss: 1.0856 - val_accuracy: 0.6024 - val_loss: 0.9948\n",
      "Epoch 38/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 375ms/step - accuracy: 0.5297 - loss: 1.0556 - val_accuracy: 0.5701 - val_loss: 0.9792\n",
      "Epoch 39/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 367ms/step - accuracy: 0.5263 - loss: 1.0724 - val_accuracy: 0.5910 - val_loss: 0.9749\n",
      "Epoch 40/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 378ms/step - accuracy: 0.5161 - loss: 1.0685 - val_accuracy: 0.5869 - val_loss: 0.9768\n",
      "Epoch 41/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 373ms/step - accuracy: 0.5237 - loss: 1.0590 - val_accuracy: 0.5836 - val_loss: 0.9621\n",
      "Epoch 42/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 370ms/step - accuracy: 0.5235 - loss: 1.0728 - val_accuracy: 0.5984 - val_loss: 0.9730\n",
      "Epoch 43/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 376ms/step - accuracy: 0.5217 - loss: 1.0561 - val_accuracy: 0.5815 - val_loss: 0.9618\n",
      "Epoch 44/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 366ms/step - accuracy: 0.5361 - loss: 1.0492 - val_accuracy: 0.5856 - val_loss: 0.9787\n",
      "Epoch 45/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 359ms/step - accuracy: 0.5395 - loss: 1.0413 - val_accuracy: 0.5290 - val_loss: 1.0086\n",
      "Epoch 46/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 369ms/step - accuracy: 0.4959 - loss: 1.0769 - val_accuracy: 0.5296 - val_loss: 1.0107\n",
      "Epoch 47/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 364ms/step - accuracy: 0.5391 - loss: 1.0503 - val_accuracy: 0.5842 - val_loss: 0.9956\n",
      "Epoch 48/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 367ms/step - accuracy: 0.5397 - loss: 1.0303 - val_accuracy: 0.5539 - val_loss: 0.9989\n",
      "Epoch 49/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 364ms/step - accuracy: 0.5236 - loss: 1.0629 - val_accuracy: 0.5822 - val_loss: 0.9651\n",
      "Epoch 50/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 370ms/step - accuracy: 0.5319 - loss: 1.0442 - val_accuracy: 0.5916 - val_loss: 0.9534\n",
      "Epoch 51/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 361ms/step - accuracy: 0.5400 - loss: 1.0372 - val_accuracy: 0.6152 - val_loss: 0.9668\n",
      "Epoch 52/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 363ms/step - accuracy: 0.5301 - loss: 1.0542 - val_accuracy: 0.5768 - val_loss: 0.9698\n",
      "Epoch 53/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 380ms/step - accuracy: 0.5445 - loss: 1.0245 - val_accuracy: 0.5782 - val_loss: 0.9663\n",
      "Epoch 54/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 384ms/step - accuracy: 0.5317 - loss: 1.0418 - val_accuracy: 0.5735 - val_loss: 0.9644\n",
      "Epoch 55/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 371ms/step - accuracy: 0.5310 - loss: 1.0370 - val_accuracy: 0.6038 - val_loss: 0.9521\n",
      "Epoch 56/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 371ms/step - accuracy: 0.5268 - loss: 1.0646 - val_accuracy: 0.6159 - val_loss: 0.9611\n",
      "Epoch 57/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 374ms/step - accuracy: 0.5325 - loss: 1.0582 - val_accuracy: 0.5923 - val_loss: 0.9552\n",
      "Epoch 58/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 371ms/step - accuracy: 0.5539 - loss: 1.0193 - val_accuracy: 0.6038 - val_loss: 0.9698\n",
      "Epoch 59/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 371ms/step - accuracy: 0.5473 - loss: 1.0269 - val_accuracy: 0.6166 - val_loss: 0.9602\n",
      "Epoch 60/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 374ms/step - accuracy: 0.5402 - loss: 1.0280 - val_accuracy: 0.5822 - val_loss: 0.9611\n",
      "Epoch 61/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 377ms/step - accuracy: 0.5317 - loss: 1.0402 - val_accuracy: 0.5910 - val_loss: 0.9454\n",
      "Epoch 62/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 366ms/step - accuracy: 0.5271 - loss: 1.0556 - val_accuracy: 0.5916 - val_loss: 0.9524\n",
      "Epoch 63/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 370ms/step - accuracy: 0.5528 - loss: 1.0189 - val_accuracy: 0.6112 - val_loss: 0.9367\n",
      "Epoch 64/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 364ms/step - accuracy: 0.5431 - loss: 1.0344 - val_accuracy: 0.6011 - val_loss: 0.9409\n",
      "Epoch 65/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 376ms/step - accuracy: 0.5563 - loss: 1.0073 - val_accuracy: 0.6166 - val_loss: 0.9383\n",
      "Epoch 66/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 379ms/step - accuracy: 0.5354 - loss: 1.0432 - val_accuracy: 0.5869 - val_loss: 0.9678\n",
      "Epoch 67/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 372ms/step - accuracy: 0.5379 - loss: 1.0268 - val_accuracy: 0.5849 - val_loss: 0.9608\n",
      "Epoch 68/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 374ms/step - accuracy: 0.5372 - loss: 1.0404 - val_accuracy: 0.5964 - val_loss: 0.9568\n",
      "Epoch 69/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 373ms/step - accuracy: 0.5468 - loss: 1.0169 - val_accuracy: 0.6098 - val_loss: 0.9861\n",
      "Epoch 70/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 388ms/step - accuracy: 0.5439 - loss: 1.0350 - val_accuracy: 0.6159 - val_loss: 0.9381\n",
      "Epoch 71/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 385ms/step - accuracy: 0.5429 - loss: 1.0235 - val_accuracy: 0.6011 - val_loss: 0.9349\n",
      "Epoch 72/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 358ms/step - accuracy: 0.5573 - loss: 1.0041 - val_accuracy: 0.6085 - val_loss: 0.9467\n",
      "Epoch 73/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 374ms/step - accuracy: 0.5469 - loss: 1.0235 - val_accuracy: 0.6267 - val_loss: 0.9439\n",
      "Epoch 74/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 369ms/step - accuracy: 0.5514 - loss: 1.0134 - val_accuracy: 0.6038 - val_loss: 0.9303\n",
      "Epoch 75/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 380ms/step - accuracy: 0.5511 - loss: 1.0125 - val_accuracy: 0.5856 - val_loss: 0.9435\n",
      "Epoch 76/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 378ms/step - accuracy: 0.5356 - loss: 1.0322 - val_accuracy: 0.5970 - val_loss: 0.9302\n",
      "Epoch 77/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 368ms/step - accuracy: 0.5433 - loss: 1.0226 - val_accuracy: 0.5546 - val_loss: 0.9820\n",
      "Epoch 78/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 378ms/step - accuracy: 0.5433 - loss: 1.0218 - val_accuracy: 0.6260 - val_loss: 0.9469\n",
      "Epoch 79/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 384ms/step - accuracy: 0.5610 - loss: 1.0346 - val_accuracy: 0.6247 - val_loss: 0.9299\n",
      "Epoch 80/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 380ms/step - accuracy: 0.5614 - loss: 1.0052 - val_accuracy: 0.6011 - val_loss: 0.9310\n",
      "Epoch 81/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 384ms/step - accuracy: 0.5594 - loss: 1.0184 - val_accuracy: 0.6247 - val_loss: 0.9180\n",
      "Epoch 82/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 364ms/step - accuracy: 0.5356 - loss: 1.0234 - val_accuracy: 0.5815 - val_loss: 0.9472\n",
      "Epoch 83/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 385ms/step - accuracy: 0.5335 - loss: 1.0224 - val_accuracy: 0.6186 - val_loss: 0.9233\n",
      "Epoch 84/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 376ms/step - accuracy: 0.5558 - loss: 1.0029 - val_accuracy: 0.5883 - val_loss: 0.9405\n",
      "Epoch 85/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 363ms/step - accuracy: 0.5542 - loss: 1.0175 - val_accuracy: 0.6024 - val_loss: 0.9577\n",
      "Epoch 86/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 366ms/step - accuracy: 0.5676 - loss: 1.0123 - val_accuracy: 0.5512 - val_loss: 1.0065\n",
      "Epoch 87/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 373ms/step - accuracy: 0.5492 - loss: 1.0204 - val_accuracy: 0.5741 - val_loss: 0.9586\n",
      "Epoch 88/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 372ms/step - accuracy: 0.5434 - loss: 1.0189 - val_accuracy: 0.6092 - val_loss: 0.9241\n",
      "Epoch 89/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 364ms/step - accuracy: 0.5523 - loss: 1.0094 - val_accuracy: 0.5970 - val_loss: 0.9299\n",
      "Epoch 90/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 370ms/step - accuracy: 0.5552 - loss: 1.0087 - val_accuracy: 0.6119 - val_loss: 0.9230\n",
      "Epoch 91/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 366ms/step - accuracy: 0.5346 - loss: 1.0147 - val_accuracy: 0.6159 - val_loss: 0.9164\n",
      "Epoch 92/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 362ms/step - accuracy: 0.5627 - loss: 0.9989 - val_accuracy: 0.5910 - val_loss: 0.9525\n",
      "Epoch 93/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 361ms/step - accuracy: 0.5479 - loss: 1.0193 - val_accuracy: 0.5991 - val_loss: 0.9304\n",
      "Epoch 94/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 359ms/step - accuracy: 0.5591 - loss: 1.0079 - val_accuracy: 0.6058 - val_loss: 0.9223\n",
      "Epoch 95/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 370ms/step - accuracy: 0.5430 - loss: 1.0083 - val_accuracy: 0.6213 - val_loss: 0.9210\n",
      "Epoch 96/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 369ms/step - accuracy: 0.5598 - loss: 1.0094 - val_accuracy: 0.6125 - val_loss: 0.9284\n",
      "Epoch 97/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 368ms/step - accuracy: 0.5600 - loss: 0.9953 - val_accuracy: 0.6139 - val_loss: 0.9105\n",
      "Epoch 98/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 377ms/step - accuracy: 0.5755 - loss: 0.9917 - val_accuracy: 0.5863 - val_loss: 0.9534\n",
      "Epoch 99/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 366ms/step - accuracy: 0.5611 - loss: 1.0139 - val_accuracy: 0.6146 - val_loss: 0.9228\n",
      "Epoch 100/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 372ms/step - accuracy: 0.5636 - loss: 1.0039 - val_accuracy: 0.5970 - val_loss: 0.9282\n",
      "Epoch 101/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 384ms/step - accuracy: 0.5602 - loss: 1.0058 - val_accuracy: 0.6206 - val_loss: 0.9097\n",
      "Epoch 102/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 371ms/step - accuracy: 0.5571 - loss: 1.0051 - val_accuracy: 0.5889 - val_loss: 0.9381\n",
      "Epoch 103/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 374ms/step - accuracy: 0.5784 - loss: 0.9763 - val_accuracy: 0.6301 - val_loss: 0.9193\n",
      "Epoch 104/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 363ms/step - accuracy: 0.5712 - loss: 0.9990 - val_accuracy: 0.6098 - val_loss: 0.9159\n",
      "Epoch 105/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 375ms/step - accuracy: 0.5681 - loss: 0.9945 - val_accuracy: 0.6092 - val_loss: 0.9186\n",
      "Epoch 106/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 371ms/step - accuracy: 0.5521 - loss: 1.0055 - val_accuracy: 0.6220 - val_loss: 0.9250\n",
      "Epoch 107/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 379ms/step - accuracy: 0.5738 - loss: 0.9880 - val_accuracy: 0.5984 - val_loss: 0.9333\n",
      "Epoch 108/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 373ms/step - accuracy: 0.5769 - loss: 0.9761 - val_accuracy: 0.6119 - val_loss: 0.9206\n",
      "Epoch 109/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 370ms/step - accuracy: 0.5486 - loss: 1.0232 - val_accuracy: 0.6065 - val_loss: 0.9133\n",
      "Epoch 110/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 373ms/step - accuracy: 0.5539 - loss: 1.0381 - val_accuracy: 0.5883 - val_loss: 0.9454\n",
      "Epoch 111/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 376ms/step - accuracy: 0.5544 - loss: 1.0264 - val_accuracy: 0.6226 - val_loss: 0.9015\n",
      "Epoch 112/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 364ms/step - accuracy: 0.5589 - loss: 1.0016 - val_accuracy: 0.6018 - val_loss: 0.9216\n",
      "Epoch 113/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 368ms/step - accuracy: 0.5637 - loss: 0.9980 - val_accuracy: 0.6173 - val_loss: 0.9096\n",
      "Epoch 114/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 366ms/step - accuracy: 0.5558 - loss: 1.0050 - val_accuracy: 0.5991 - val_loss: 0.9363\n",
      "Epoch 115/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 374ms/step - accuracy: 0.5598 - loss: 0.9850 - val_accuracy: 0.6206 - val_loss: 0.9038\n",
      "Epoch 116/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 371ms/step - accuracy: 0.5618 - loss: 0.9902 - val_accuracy: 0.6193 - val_loss: 0.9106\n",
      "Epoch 117/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 385ms/step - accuracy: 0.5787 - loss: 0.9876 - val_accuracy: 0.6334 - val_loss: 0.9020\n",
      "Epoch 118/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 367ms/step - accuracy: 0.5560 - loss: 0.9987 - val_accuracy: 0.6105 - val_loss: 0.9192\n",
      "Epoch 119/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 360ms/step - accuracy: 0.5679 - loss: 0.9948 - val_accuracy: 0.6105 - val_loss: 0.9075\n",
      "Epoch 120/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 371ms/step - accuracy: 0.5535 - loss: 0.9970 - val_accuracy: 0.6193 - val_loss: 0.9082\n",
      "Epoch 121/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 367ms/step - accuracy: 0.5833 - loss: 0.9835 - val_accuracy: 0.6240 - val_loss: 0.9003\n",
      "Epoch 122/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 373ms/step - accuracy: 0.5618 - loss: 0.9899 - val_accuracy: 0.6112 - val_loss: 0.9085\n",
      "Epoch 123/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 363ms/step - accuracy: 0.5735 - loss: 0.9714 - val_accuracy: 0.6186 - val_loss: 0.9048\n",
      "Epoch 124/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 383ms/step - accuracy: 0.5831 - loss: 0.9686 - val_accuracy: 0.6280 - val_loss: 0.8914\n",
      "Epoch 125/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 368ms/step - accuracy: 0.5605 - loss: 0.9808 - val_accuracy: 0.5829 - val_loss: 0.9397\n",
      "Epoch 126/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 367ms/step - accuracy: 0.5749 - loss: 0.9851 - val_accuracy: 0.6260 - val_loss: 0.9034\n",
      "Epoch 127/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 382ms/step - accuracy: 0.5706 - loss: 0.9890 - val_accuracy: 0.6186 - val_loss: 0.9045\n",
      "Epoch 128/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 369ms/step - accuracy: 0.5513 - loss: 1.0209 - val_accuracy: 0.6267 - val_loss: 0.8933\n",
      "Epoch 129/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 361ms/step - accuracy: 0.5798 - loss: 0.9727 - val_accuracy: 0.5937 - val_loss: 0.9499\n",
      "Epoch 130/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 374ms/step - accuracy: 0.5629 - loss: 0.9816 - val_accuracy: 0.6247 - val_loss: 0.9100\n",
      "Epoch 131/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 369ms/step - accuracy: 0.5708 - loss: 0.9814 - val_accuracy: 0.5957 - val_loss: 0.9234\n",
      "Epoch 132/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 378ms/step - accuracy: 0.5735 - loss: 0.9812 - val_accuracy: 0.6051 - val_loss: 0.9112\n",
      "Epoch 133/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 364ms/step - accuracy: 0.5588 - loss: 1.0008 - val_accuracy: 0.6274 - val_loss: 0.8979\n",
      "Epoch 134/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 367ms/step - accuracy: 0.5743 - loss: 0.9723 - val_accuracy: 0.6381 - val_loss: 0.9113\n",
      "Epoch 135/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 364ms/step - accuracy: 0.5734 - loss: 0.9809 - val_accuracy: 0.6125 - val_loss: 0.9082\n",
      "Epoch 136/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 362ms/step - accuracy: 0.5776 - loss: 0.9776 - val_accuracy: 0.5896 - val_loss: 0.9405\n",
      "Epoch 137/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 368ms/step - accuracy: 0.5739 - loss: 0.9773 - val_accuracy: 0.6422 - val_loss: 0.8824\n",
      "Epoch 138/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 360ms/step - accuracy: 0.5817 - loss: 0.9920 - val_accuracy: 0.6327 - val_loss: 0.9028\n",
      "Epoch 139/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 366ms/step - accuracy: 0.5591 - loss: 0.9790 - val_accuracy: 0.6476 - val_loss: 0.8856\n",
      "Epoch 140/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 360ms/step - accuracy: 0.5598 - loss: 0.9871 - val_accuracy: 0.6078 - val_loss: 0.9099\n",
      "Epoch 141/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 354ms/step - accuracy: 0.5697 - loss: 0.9795 - val_accuracy: 0.5977 - val_loss: 0.9292\n",
      "Epoch 142/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 367ms/step - accuracy: 0.5697 - loss: 0.9778 - val_accuracy: 0.6173 - val_loss: 0.8963\n",
      "Epoch 143/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 366ms/step - accuracy: 0.5694 - loss: 0.9910 - val_accuracy: 0.6280 - val_loss: 0.9023\n",
      "Epoch 144/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 361ms/step - accuracy: 0.5751 - loss: 0.9764 - val_accuracy: 0.6267 - val_loss: 0.8952\n",
      "Epoch 145/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 358ms/step - accuracy: 0.5628 - loss: 0.9951 - val_accuracy: 0.6456 - val_loss: 0.8819\n",
      "Epoch 146/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 366ms/step - accuracy: 0.5759 - loss: 0.9727 - val_accuracy: 0.6287 - val_loss: 0.8770\n",
      "Epoch 147/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 367ms/step - accuracy: 0.5837 - loss: 0.9640 - val_accuracy: 0.6301 - val_loss: 0.8950\n",
      "Epoch 148/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 361ms/step - accuracy: 0.5779 - loss: 0.9714 - val_accuracy: 0.6267 - val_loss: 0.8861\n",
      "Epoch 149/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 371ms/step - accuracy: 0.5721 - loss: 0.9735 - val_accuracy: 0.5957 - val_loss: 0.9162\n",
      "Epoch 150/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 354ms/step - accuracy: 0.5743 - loss: 0.9787 - val_accuracy: 0.6193 - val_loss: 0.8946\n",
      "Epoch 151/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 359ms/step - accuracy: 0.5822 - loss: 0.9650 - val_accuracy: 0.6206 - val_loss: 0.8894\n",
      "Epoch 152/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 356ms/step - accuracy: 0.5712 - loss: 0.9751 - val_accuracy: 0.6462 - val_loss: 0.8857\n",
      "Epoch 153/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 361ms/step - accuracy: 0.5770 - loss: 0.9836 - val_accuracy: 0.6233 - val_loss: 0.8904\n",
      "Epoch 154/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 365ms/step - accuracy: 0.5601 - loss: 0.9823 - val_accuracy: 0.6186 - val_loss: 0.8937\n",
      "Epoch 155/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 367ms/step - accuracy: 0.5735 - loss: 0.9843 - val_accuracy: 0.6274 - val_loss: 0.8951\n",
      "Epoch 156/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 366ms/step - accuracy: 0.5825 - loss: 0.9750 - val_accuracy: 0.6044 - val_loss: 0.9062\n",
      "Epoch 157/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 352ms/step - accuracy: 0.5615 - loss: 0.9769 - val_accuracy: 0.6476 - val_loss: 0.8775\n",
      "Epoch 158/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 365ms/step - accuracy: 0.5715 - loss: 0.9642 - val_accuracy: 0.6536 - val_loss: 0.8706\n",
      "Epoch 159/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 362ms/step - accuracy: 0.5924 - loss: 0.9634 - val_accuracy: 0.5869 - val_loss: 0.9343\n",
      "Epoch 160/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 359ms/step - accuracy: 0.5684 - loss: 0.9746 - val_accuracy: 0.6078 - val_loss: 0.9055\n",
      "Epoch 161/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 367ms/step - accuracy: 0.5740 - loss: 0.9671 - val_accuracy: 0.6402 - val_loss: 0.8718\n",
      "Epoch 162/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 362ms/step - accuracy: 0.5922 - loss: 0.9571 - val_accuracy: 0.6301 - val_loss: 0.8742\n",
      "Epoch 163/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 364ms/step - accuracy: 0.5651 - loss: 0.9856 - val_accuracy: 0.6523 - val_loss: 0.8756\n",
      "Epoch 164/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 363ms/step - accuracy: 0.5824 - loss: 0.9680 - val_accuracy: 0.6186 - val_loss: 0.8862\n",
      "Epoch 165/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 366ms/step - accuracy: 0.5758 - loss: 0.9625 - val_accuracy: 0.6199 - val_loss: 0.8947\n",
      "Epoch 166/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 357ms/step - accuracy: 0.5721 - loss: 0.9816 - val_accuracy: 0.6462 - val_loss: 0.8721\n",
      "Epoch 167/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 359ms/step - accuracy: 0.5843 - loss: 0.9544 - val_accuracy: 0.6139 - val_loss: 0.9025\n",
      "Epoch 168/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 362ms/step - accuracy: 0.6001 - loss: 0.9443 - val_accuracy: 0.6422 - val_loss: 0.8752\n",
      "Epoch 169/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 368ms/step - accuracy: 0.5802 - loss: 0.9714 - val_accuracy: 0.6179 - val_loss: 0.8910\n",
      "Epoch 170/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 371ms/step - accuracy: 0.5916 - loss: 0.9492 - val_accuracy: 0.6476 - val_loss: 0.8771\n",
      "Epoch 171/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 368ms/step - accuracy: 0.5848 - loss: 0.9813 - val_accuracy: 0.5950 - val_loss: 0.9343\n",
      "Epoch 172/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 357ms/step - accuracy: 0.5687 - loss: 0.9742 - val_accuracy: 0.6429 - val_loss: 0.8726\n",
      "Epoch 173/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 365ms/step - accuracy: 0.5645 - loss: 0.9793 - val_accuracy: 0.6321 - val_loss: 0.8826\n",
      "Epoch 174/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 366ms/step - accuracy: 0.5947 - loss: 0.9556 - val_accuracy: 0.6334 - val_loss: 0.8684\n",
      "Epoch 175/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 376ms/step - accuracy: 0.5885 - loss: 0.9625 - val_accuracy: 0.6503 - val_loss: 0.8703\n",
      "Epoch 176/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 368ms/step - accuracy: 0.5842 - loss: 0.9699 - val_accuracy: 0.6314 - val_loss: 0.8826\n",
      "Epoch 177/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 362ms/step - accuracy: 0.5984 - loss: 0.9647 - val_accuracy: 0.6570 - val_loss: 0.8626\n",
      "Epoch 178/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 359ms/step - accuracy: 0.6029 - loss: 0.9426 - val_accuracy: 0.6429 - val_loss: 0.8717\n",
      "Epoch 179/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 360ms/step - accuracy: 0.5846 - loss: 0.9620 - val_accuracy: 0.6496 - val_loss: 0.8770\n",
      "Epoch 180/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 359ms/step - accuracy: 0.5758 - loss: 0.9903 - val_accuracy: 0.6240 - val_loss: 0.8864\n",
      "Epoch 181/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 359ms/step - accuracy: 0.5817 - loss: 0.9646 - val_accuracy: 0.6368 - val_loss: 0.8666\n",
      "Epoch 182/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 359ms/step - accuracy: 0.5965 - loss: 0.9566 - val_accuracy: 0.6375 - val_loss: 0.8634\n",
      "Epoch 183/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 365ms/step - accuracy: 0.5877 - loss: 0.9516 - val_accuracy: 0.6449 - val_loss: 0.8589\n",
      "Epoch 184/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 363ms/step - accuracy: 0.5855 - loss: 0.9578 - val_accuracy: 0.6078 - val_loss: 0.9061\n",
      "Epoch 185/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 366ms/step - accuracy: 0.5796 - loss: 0.9616 - val_accuracy: 0.6422 - val_loss: 0.8806\n",
      "Epoch 186/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 352ms/step - accuracy: 0.5969 - loss: 0.9560 - val_accuracy: 0.6186 - val_loss: 0.8822\n",
      "Epoch 187/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 355ms/step - accuracy: 0.5847 - loss: 0.9627 - val_accuracy: 0.6334 - val_loss: 0.8692\n",
      "Epoch 188/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 361ms/step - accuracy: 0.5998 - loss: 0.9417 - val_accuracy: 0.6523 - val_loss: 0.8665\n",
      "Epoch 189/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 373ms/step - accuracy: 0.5933 - loss: 0.9442 - val_accuracy: 0.6226 - val_loss: 0.8823\n",
      "Epoch 190/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 359ms/step - accuracy: 0.5731 - loss: 0.9673 - val_accuracy: 0.6408 - val_loss: 0.8614\n",
      "Epoch 191/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 375ms/step - accuracy: 0.5720 - loss: 0.9546 - val_accuracy: 0.6402 - val_loss: 0.8645\n",
      "Epoch 192/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 371ms/step - accuracy: 0.5971 - loss: 0.9469 - val_accuracy: 0.6327 - val_loss: 0.8730\n",
      "Epoch 193/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 375ms/step - accuracy: 0.5828 - loss: 0.9553 - val_accuracy: 0.6489 - val_loss: 0.8542\n",
      "Epoch 194/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 360ms/step - accuracy: 0.5724 - loss: 0.9665 - val_accuracy: 0.6482 - val_loss: 0.8544\n",
      "Epoch 195/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 362ms/step - accuracy: 0.5841 - loss: 0.9839 - val_accuracy: 0.6368 - val_loss: 0.8651\n",
      "Epoch 196/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 358ms/step - accuracy: 0.5780 - loss: 0.9660 - val_accuracy: 0.6078 - val_loss: 0.9230\n",
      "Epoch 197/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 364ms/step - accuracy: 0.5991 - loss: 0.9575 - val_accuracy: 0.6651 - val_loss: 0.8539\n",
      "Epoch 198/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 361ms/step - accuracy: 0.5802 - loss: 0.9537 - val_accuracy: 0.6597 - val_loss: 0.8494\n",
      "Epoch 199/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 354ms/step - accuracy: 0.5956 - loss: 0.9589 - val_accuracy: 0.6489 - val_loss: 0.8583\n",
      "Epoch 200/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 365ms/step - accuracy: 0.5967 - loss: 0.9588 - val_accuracy: 0.6577 - val_loss: 0.8572\n",
      "Epoch 201/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 365ms/step - accuracy: 0.5866 - loss: 0.9513 - val_accuracy: 0.6146 - val_loss: 0.9040\n",
      "Epoch 202/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 363ms/step - accuracy: 0.5848 - loss: 0.9609 - val_accuracy: 0.6664 - val_loss: 0.8587\n",
      "Epoch 203/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 359ms/step - accuracy: 0.5859 - loss: 0.9528 - val_accuracy: 0.5883 - val_loss: 0.9317\n",
      "Epoch 204/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 360ms/step - accuracy: 0.5665 - loss: 0.9850 - val_accuracy: 0.6395 - val_loss: 0.8795\n",
      "Epoch 205/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 361ms/step - accuracy: 0.5882 - loss: 0.9467 - val_accuracy: 0.6584 - val_loss: 0.8561\n",
      "Epoch 206/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 364ms/step - accuracy: 0.6060 - loss: 0.9297 - val_accuracy: 0.6280 - val_loss: 0.8630\n",
      "Epoch 207/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 355ms/step - accuracy: 0.5832 - loss: 0.9566 - val_accuracy: 0.6105 - val_loss: 0.8924\n",
      "Epoch 208/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 364ms/step - accuracy: 0.5851 - loss: 0.9586 - val_accuracy: 0.6476 - val_loss: 0.8563\n",
      "Epoch 209/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 355ms/step - accuracy: 0.5950 - loss: 0.9544 - val_accuracy: 0.6469 - val_loss: 0.8593\n",
      "Epoch 210/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 354ms/step - accuracy: 0.5827 - loss: 0.9625 - val_accuracy: 0.6119 - val_loss: 0.8940\n",
      "Epoch 211/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 363ms/step - accuracy: 0.5864 - loss: 0.9519 - val_accuracy: 0.6354 - val_loss: 0.8650\n",
      "Epoch 212/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 360ms/step - accuracy: 0.5828 - loss: 0.9523 - val_accuracy: 0.6577 - val_loss: 0.8451\n",
      "Epoch 213/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 355ms/step - accuracy: 0.5963 - loss: 0.9359 - val_accuracy: 0.6415 - val_loss: 0.8621\n",
      "Epoch 214/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 365ms/step - accuracy: 0.5982 - loss: 0.9400 - val_accuracy: 0.6173 - val_loss: 0.8826\n",
      "Epoch 215/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 350ms/step - accuracy: 0.5752 - loss: 0.9632 - val_accuracy: 0.6388 - val_loss: 0.8657\n",
      "Epoch 216/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 356ms/step - accuracy: 0.5955 - loss: 0.9451 - val_accuracy: 0.6287 - val_loss: 0.8737\n",
      "Epoch 217/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 358ms/step - accuracy: 0.5990 - loss: 0.9470 - val_accuracy: 0.6611 - val_loss: 0.8486\n",
      "Epoch 218/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 361ms/step - accuracy: 0.5924 - loss: 0.9497 - val_accuracy: 0.6624 - val_loss: 0.8496\n",
      "Epoch 219/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 366ms/step - accuracy: 0.5954 - loss: 0.9535 - val_accuracy: 0.6381 - val_loss: 0.8635\n",
      "Epoch 220/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 357ms/step - accuracy: 0.5971 - loss: 0.9395 - val_accuracy: 0.6233 - val_loss: 0.8864\n",
      "Epoch 221/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 356ms/step - accuracy: 0.5864 - loss: 0.9495 - val_accuracy: 0.6220 - val_loss: 0.8731\n",
      "Epoch 222/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 357ms/step - accuracy: 0.5879 - loss: 0.9428 - val_accuracy: 0.6712 - val_loss: 0.8593\n",
      "Epoch 223/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 356ms/step - accuracy: 0.6009 - loss: 0.9291 - val_accuracy: 0.5957 - val_loss: 0.9165\n",
      "Epoch 224/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 370ms/step - accuracy: 0.5903 - loss: 0.9462 - val_accuracy: 0.6631 - val_loss: 0.8413\n",
      "Epoch 225/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 363ms/step - accuracy: 0.5981 - loss: 0.9286 - val_accuracy: 0.6435 - val_loss: 0.8567\n",
      "Epoch 226/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 358ms/step - accuracy: 0.5878 - loss: 0.9630 - val_accuracy: 0.6550 - val_loss: 0.8512\n",
      "Epoch 227/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 358ms/step - accuracy: 0.5862 - loss: 0.9571 - val_accuracy: 0.6691 - val_loss: 0.8501\n",
      "Epoch 228/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 362ms/step - accuracy: 0.5956 - loss: 0.9527 - val_accuracy: 0.5937 - val_loss: 0.9169\n",
      "Epoch 229/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 353ms/step - accuracy: 0.5828 - loss: 0.9387 - val_accuracy: 0.6604 - val_loss: 0.8640\n",
      "Epoch 230/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 362ms/step - accuracy: 0.5872 - loss: 0.9442 - val_accuracy: 0.6671 - val_loss: 0.8404\n",
      "Epoch 231/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 362ms/step - accuracy: 0.5980 - loss: 0.9499 - val_accuracy: 0.6604 - val_loss: 0.8490\n",
      "Epoch 232/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 357ms/step - accuracy: 0.5912 - loss: 0.9454 - val_accuracy: 0.5930 - val_loss: 0.9134\n",
      "Epoch 233/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 364ms/step - accuracy: 0.5987 - loss: 0.9326 - val_accuracy: 0.6637 - val_loss: 0.8414\n",
      "Epoch 234/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 369ms/step - accuracy: 0.5799 - loss: 0.9547 - val_accuracy: 0.5721 - val_loss: 0.9945\n",
      "Epoch 235/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 353ms/step - accuracy: 0.5848 - loss: 0.9461 - val_accuracy: 0.6334 - val_loss: 0.8707\n",
      "Epoch 236/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 361ms/step - accuracy: 0.5902 - loss: 0.9437 - val_accuracy: 0.6604 - val_loss: 0.8373\n",
      "Epoch 237/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 352ms/step - accuracy: 0.5716 - loss: 0.9680 - val_accuracy: 0.6739 - val_loss: 0.8430\n",
      "Epoch 238/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 355ms/step - accuracy: 0.6045 - loss: 0.9286 - val_accuracy: 0.6368 - val_loss: 0.8630\n",
      "Epoch 239/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 363ms/step - accuracy: 0.5903 - loss: 0.9321 - val_accuracy: 0.6597 - val_loss: 0.8388\n",
      "Epoch 240/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 356ms/step - accuracy: 0.5957 - loss: 0.9343 - val_accuracy: 0.6496 - val_loss: 0.8404\n",
      "Epoch 241/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 362ms/step - accuracy: 0.5881 - loss: 0.9522 - val_accuracy: 0.6321 - val_loss: 0.8583\n",
      "Epoch 242/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 353ms/step - accuracy: 0.5771 - loss: 0.9496 - val_accuracy: 0.5916 - val_loss: 0.9086\n",
      "Epoch 243/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 360ms/step - accuracy: 0.5859 - loss: 0.9438 - val_accuracy: 0.6274 - val_loss: 0.8738\n",
      "Epoch 244/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 355ms/step - accuracy: 0.6056 - loss: 0.9301 - val_accuracy: 0.6590 - val_loss: 0.8518\n",
      "Epoch 245/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 345ms/step - accuracy: 0.6135 - loss: 0.9341 - val_accuracy: 0.6550 - val_loss: 0.8414\n",
      "Epoch 246/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 355ms/step - accuracy: 0.6009 - loss: 0.9360 - val_accuracy: 0.6503 - val_loss: 0.8493\n",
      "Epoch 247/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 352ms/step - accuracy: 0.6066 - loss: 0.9242 - val_accuracy: 0.6348 - val_loss: 0.8589\n",
      "Epoch 248/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 356ms/step - accuracy: 0.5858 - loss: 0.9619 - val_accuracy: 0.6307 - val_loss: 0.8627\n",
      "Epoch 249/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 366ms/step - accuracy: 0.6103 - loss: 0.9356 - val_accuracy: 0.6637 - val_loss: 0.8341\n",
      "Epoch 250/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 358ms/step - accuracy: 0.5995 - loss: 0.9232 - val_accuracy: 0.6213 - val_loss: 0.8913\n",
      "Epoch 1/250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-28 23:47:29.324606: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_15690', 76 bytes spill stores, 76 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 625ms/step - accuracy: 0.4062 - loss: 38.0377 - val_accuracy: 0.2487 - val_loss: 35.3444\n",
      "Epoch 2/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 355ms/step - accuracy: 0.7646 - loss: 1.4022 - val_accuracy: 0.2487 - val_loss: 26.5567\n",
      "Epoch 3/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 347ms/step - accuracy: 0.8274 - loss: 0.7819 - val_accuracy: 0.2487 - val_loss: 35.6060\n",
      "Epoch 4/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 355ms/step - accuracy: 0.8545 - loss: 0.5429 - val_accuracy: 0.2513 - val_loss: 31.0879\n",
      "Epoch 5/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 349ms/step - accuracy: 0.8785 - loss: 0.3898 - val_accuracy: 0.4144 - val_loss: 8.9663\n",
      "Epoch 6/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 357ms/step - accuracy: 0.8996 - loss: 0.3316 - val_accuracy: 0.7102 - val_loss: 1.7920\n",
      "Epoch 7/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 372ms/step - accuracy: 0.9158 - loss: 0.2448 - val_accuracy: 0.8673 - val_loss: 0.4938\n",
      "Epoch 8/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 368ms/step - accuracy: 0.9308 - loss: 0.2101 - val_accuracy: 0.9178 - val_loss: 0.2172\n",
      "Epoch 9/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 362ms/step - accuracy: 0.9302 - loss: 0.2113 - val_accuracy: 0.9488 - val_loss: 0.1443\n",
      "Epoch 10/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 373ms/step - accuracy: 0.9450 - loss: 0.1767 - val_accuracy: 0.9629 - val_loss: 0.1056\n",
      "Epoch 11/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 368ms/step - accuracy: 0.9429 - loss: 0.1536 - val_accuracy: 0.9596 - val_loss: 0.1031\n",
      "Epoch 12/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 368ms/step - accuracy: 0.9563 - loss: 0.1243 - val_accuracy: 0.9656 - val_loss: 0.0850\n",
      "Epoch 13/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 363ms/step - accuracy: 0.9587 - loss: 0.1113 - val_accuracy: 0.9683 - val_loss: 0.0849\n",
      "Epoch 14/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 368ms/step - accuracy: 0.9670 - loss: 0.1050 - val_accuracy: 0.9730 - val_loss: 0.0764\n",
      "Epoch 15/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 369ms/step - accuracy: 0.9717 - loss: 0.0867 - val_accuracy: 0.9744 - val_loss: 0.0715\n",
      "Epoch 16/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 365ms/step - accuracy: 0.9691 - loss: 0.0796 - val_accuracy: 0.9798 - val_loss: 0.0644\n",
      "Epoch 17/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 362ms/step - accuracy: 0.9731 - loss: 0.0717 - val_accuracy: 0.9805 - val_loss: 0.0479\n",
      "Epoch 18/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 351ms/step - accuracy: 0.9740 - loss: 0.0765 - val_accuracy: 0.9798 - val_loss: 0.0485\n",
      "Epoch 19/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 366ms/step - accuracy: 0.9777 - loss: 0.0678 - val_accuracy: 0.9838 - val_loss: 0.0405\n",
      "Epoch 20/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 361ms/step - accuracy: 0.9776 - loss: 0.0543 - val_accuracy: 0.9798 - val_loss: 0.0535\n",
      "Epoch 21/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 377ms/step - accuracy: 0.9785 - loss: 0.0638 - val_accuracy: 0.9885 - val_loss: 0.0352\n",
      "Epoch 22/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 358ms/step - accuracy: 0.9809 - loss: 0.0515 - val_accuracy: 0.9858 - val_loss: 0.0375\n",
      "Epoch 23/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 362ms/step - accuracy: 0.9796 - loss: 0.0588 - val_accuracy: 0.9899 - val_loss: 0.0290\n",
      "Epoch 24/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 354ms/step - accuracy: 0.9843 - loss: 0.0427 - val_accuracy: 0.9771 - val_loss: 0.0719\n",
      "Epoch 25/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 364ms/step - accuracy: 0.9841 - loss: 0.0511 - val_accuracy: 0.9838 - val_loss: 0.0487\n",
      "Epoch 26/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 363ms/step - accuracy: 0.9869 - loss: 0.0374 - val_accuracy: 0.9912 - val_loss: 0.0242\n",
      "Epoch 27/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 378ms/step - accuracy: 0.9887 - loss: 0.0426 - val_accuracy: 0.9939 - val_loss: 0.0187\n",
      "Epoch 28/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 367ms/step - accuracy: 0.9856 - loss: 0.0736 - val_accuracy: 0.9899 - val_loss: 0.0218\n",
      "Epoch 29/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 359ms/step - accuracy: 0.9888 - loss: 0.0409 - val_accuracy: 0.9912 - val_loss: 0.0204\n",
      "Epoch 30/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 370ms/step - accuracy: 0.9891 - loss: 0.0300 - val_accuracy: 0.9933 - val_loss: 0.0189\n",
      "Epoch 31/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 358ms/step - accuracy: 0.9908 - loss: 0.0318 - val_accuracy: 0.9912 - val_loss: 0.0191\n",
      "Epoch 32/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 351ms/step - accuracy: 0.9890 - loss: 0.0381 - val_accuracy: 0.9872 - val_loss: 0.0265\n",
      "Epoch 33/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 354ms/step - accuracy: 0.9926 - loss: 0.0245 - val_accuracy: 0.9757 - val_loss: 0.0786\n",
      "Epoch 34/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 352ms/step - accuracy: 0.9906 - loss: 0.0302 - val_accuracy: 0.9912 - val_loss: 0.0233\n",
      "Epoch 35/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 364ms/step - accuracy: 0.9890 - loss: 0.0285 - val_accuracy: 0.9953 - val_loss: 0.0120\n",
      "Epoch 36/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 365ms/step - accuracy: 0.9915 - loss: 0.0221 - val_accuracy: 0.9966 - val_loss: 0.0099\n",
      "Epoch 37/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 371ms/step - accuracy: 0.9928 - loss: 0.0226 - val_accuracy: 0.9960 - val_loss: 0.0095\n",
      "Epoch 38/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 355ms/step - accuracy: 0.9930 - loss: 0.0302 - val_accuracy: 0.9933 - val_loss: 0.0187\n",
      "Epoch 39/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 363ms/step - accuracy: 0.9976 - loss: 0.0109 - val_accuracy: 0.9939 - val_loss: 0.0147\n",
      "Epoch 40/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 356ms/step - accuracy: 0.9954 - loss: 0.0140 - val_accuracy: 0.9966 - val_loss: 0.0151\n",
      "Epoch 41/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 355ms/step - accuracy: 0.9910 - loss: 0.0336 - val_accuracy: 0.9926 - val_loss: 0.0213\n",
      "Epoch 42/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 353ms/step - accuracy: 0.9939 - loss: 0.0200 - val_accuracy: 0.9872 - val_loss: 0.0593\n",
      "Epoch 43/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 368ms/step - accuracy: 0.9933 - loss: 0.0208 - val_accuracy: 0.9953 - val_loss: 0.0084\n",
      "Epoch 44/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 362ms/step - accuracy: 0.9940 - loss: 0.0225 - val_accuracy: 0.9987 - val_loss: 0.0043\n",
      "Epoch 45/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 348ms/step - accuracy: 0.9945 - loss: 0.0170 - val_accuracy: 0.9906 - val_loss: 0.0309\n",
      "Epoch 46/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 351ms/step - accuracy: 0.9925 - loss: 0.0203 - val_accuracy: 0.9973 - val_loss: 0.0080\n",
      "Epoch 47/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 362ms/step - accuracy: 0.9959 - loss: 0.0113 - val_accuracy: 0.9933 - val_loss: 0.0249\n",
      "Epoch 48/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 362ms/step - accuracy: 0.9934 - loss: 0.0210 - val_accuracy: 0.9939 - val_loss: 0.0184\n",
      "Epoch 49/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 369ms/step - accuracy: 0.9931 - loss: 0.0151 - val_accuracy: 0.9899 - val_loss: 0.0357\n",
      "Epoch 50/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 359ms/step - accuracy: 0.9953 - loss: 0.0129 - val_accuracy: 0.9960 - val_loss: 0.0159\n",
      "Epoch 51/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 350ms/step - accuracy: 0.9949 - loss: 0.0158 - val_accuracy: 0.9966 - val_loss: 0.0114\n",
      "Epoch 52/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 358ms/step - accuracy: 0.9952 - loss: 0.0192 - val_accuracy: 0.9906 - val_loss: 0.0239\n",
      "Epoch 53/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 356ms/step - accuracy: 0.9932 - loss: 0.0153 - val_accuracy: 0.9778 - val_loss: 0.0858\n",
      "Epoch 54/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 357ms/step - accuracy: 0.9950 - loss: 0.0162 - val_accuracy: 0.9879 - val_loss: 0.0338\n",
      "Epoch 55/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 350ms/step - accuracy: 0.9948 - loss: 0.0159 - val_accuracy: 0.9939 - val_loss: 0.0131\n",
      "Epoch 56/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 355ms/step - accuracy: 0.9964 - loss: 0.0214 - val_accuracy: 0.9960 - val_loss: 0.0060\n",
      "Epoch 57/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 350ms/step - accuracy: 0.9955 - loss: 0.0133 - val_accuracy: 0.9980 - val_loss: 0.0051\n",
      "Epoch 58/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 356ms/step - accuracy: 0.9949 - loss: 0.0123 - val_accuracy: 0.9987 - val_loss: 0.0071\n",
      "Epoch 59/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 354ms/step - accuracy: 0.9939 - loss: 0.0158 - val_accuracy: 0.9946 - val_loss: 0.0207\n",
      "Epoch 60/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 354ms/step - accuracy: 0.9969 - loss: 0.0076 - val_accuracy: 0.9966 - val_loss: 0.0113\n",
      "Epoch 61/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 368ms/step - accuracy: 0.9981 - loss: 0.0065 - val_accuracy: 0.9987 - val_loss: 0.0035\n",
      "Epoch 62/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 365ms/step - accuracy: 0.9953 - loss: 0.0207 - val_accuracy: 0.9987 - val_loss: 0.0062\n",
      "Epoch 63/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 358ms/step - accuracy: 0.9967 - loss: 0.0125 - val_accuracy: 0.9973 - val_loss: 0.0173\n",
      "Epoch 64/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 353ms/step - accuracy: 0.9946 - loss: 0.0151 - val_accuracy: 0.9973 - val_loss: 0.0115\n",
      "Epoch 65/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 363ms/step - accuracy: 0.9978 - loss: 0.0087 - val_accuracy: 0.9953 - val_loss: 0.0158\n",
      "Epoch 66/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 367ms/step - accuracy: 0.9953 - loss: 0.0127 - val_accuracy: 0.9973 - val_loss: 0.0041\n",
      "Epoch 67/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 349ms/step - accuracy: 0.9974 - loss: 0.0085 - val_accuracy: 0.9973 - val_loss: 0.0059\n",
      "Epoch 68/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 351ms/step - accuracy: 0.9963 - loss: 0.0113 - val_accuracy: 0.9993 - val_loss: 0.0061\n",
      "Epoch 69/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 357ms/step - accuracy: 0.9949 - loss: 0.0140 - val_accuracy: 0.9858 - val_loss: 0.0493\n",
      "Epoch 70/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 359ms/step - accuracy: 0.9965 - loss: 0.0096 - val_accuracy: 0.9966 - val_loss: 0.0103\n",
      "Epoch 71/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 355ms/step - accuracy: 0.9963 - loss: 0.0111 - val_accuracy: 0.9987 - val_loss: 0.0069\n",
      "Epoch 72/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 362ms/step - accuracy: 0.9956 - loss: 0.0123 - val_accuracy: 0.9926 - val_loss: 0.0168\n",
      "Epoch 73/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 356ms/step - accuracy: 0.9965 - loss: 0.0185 - val_accuracy: 0.9987 - val_loss: 0.0072\n",
      "Epoch 74/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 349ms/step - accuracy: 0.9962 - loss: 0.0087 - val_accuracy: 0.9919 - val_loss: 0.0239\n",
      "Epoch 75/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 356ms/step - accuracy: 0.9969 - loss: 0.0090 - val_accuracy: 0.9939 - val_loss: 0.0196\n",
      "Epoch 76/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 356ms/step - accuracy: 0.9968 - loss: 0.0130 - val_accuracy: 0.9960 - val_loss: 0.0138\n",
      "Epoch 77/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 358ms/step - accuracy: 0.9951 - loss: 0.0170 - val_accuracy: 0.9960 - val_loss: 0.0104\n",
      "Epoch 78/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 364ms/step - accuracy: 0.9965 - loss: 0.0099 - val_accuracy: 0.9933 - val_loss: 0.0162\n",
      "Epoch 79/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 355ms/step - accuracy: 0.9968 - loss: 0.0080 - val_accuracy: 0.9980 - val_loss: 0.0057\n",
      "Epoch 80/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 362ms/step - accuracy: 0.9983 - loss: 0.0042 - val_accuracy: 0.9993 - val_loss: 0.0027\n",
      "Epoch 81/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 355ms/step - accuracy: 0.9966 - loss: 0.0185 - val_accuracy: 0.9987 - val_loss: 0.0046\n",
      "Epoch 82/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 368ms/step - accuracy: 0.9978 - loss: 0.0051 - val_accuracy: 1.0000 - val_loss: 6.6502e-04\n",
      "Epoch 83/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 355ms/step - accuracy: 0.9986 - loss: 0.0031 - val_accuracy: 0.9980 - val_loss: 0.0037\n",
      "Epoch 84/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 355ms/step - accuracy: 0.9977 - loss: 0.0088 - val_accuracy: 0.9987 - val_loss: 0.0034\n",
      "Epoch 85/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 363ms/step - accuracy: 0.9976 - loss: 0.0043 - val_accuracy: 0.9966 - val_loss: 0.0060\n",
      "Epoch 86/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 356ms/step - accuracy: 0.9991 - loss: 0.0038 - val_accuracy: 0.9966 - val_loss: 0.0056\n",
      "Epoch 87/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 363ms/step - accuracy: 0.9969 - loss: 0.0098 - val_accuracy: 0.9960 - val_loss: 0.0094\n",
      "Epoch 88/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 354ms/step - accuracy: 0.9982 - loss: 0.0046 - val_accuracy: 0.9987 - val_loss: 0.0045\n",
      "Epoch 89/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 361ms/step - accuracy: 0.9988 - loss: 0.0052 - val_accuracy: 0.9973 - val_loss: 0.0122\n",
      "Epoch 90/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 366ms/step - accuracy: 0.9957 - loss: 0.0105 - val_accuracy: 0.9980 - val_loss: 0.0030\n",
      "Epoch 91/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 363ms/step - accuracy: 0.9961 - loss: 0.0141 - val_accuracy: 0.9906 - val_loss: 0.0425\n",
      "Epoch 92/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 353ms/step - accuracy: 0.9973 - loss: 0.0089 - val_accuracy: 0.9993 - val_loss: 0.0028\n",
      "Epoch 93/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 353ms/step - accuracy: 0.9984 - loss: 0.0050 - val_accuracy: 0.9980 - val_loss: 0.0045\n",
      "Epoch 94/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 361ms/step - accuracy: 0.9991 - loss: 0.0038 - val_accuracy: 0.9993 - val_loss: 7.5294e-04\n",
      "Epoch 95/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 356ms/step - accuracy: 0.9971 - loss: 0.0083 - val_accuracy: 0.9960 - val_loss: 0.0149\n",
      "Epoch 96/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 369ms/step - accuracy: 0.9971 - loss: 0.0107 - val_accuracy: 0.9960 - val_loss: 0.0099\n",
      "Epoch 97/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 355ms/step - accuracy: 0.9969 - loss: 0.0068 - val_accuracy: 0.9993 - val_loss: 0.0014\n",
      "Epoch 98/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 358ms/step - accuracy: 0.9984 - loss: 0.0043 - val_accuracy: 0.9987 - val_loss: 0.0044\n",
      "Epoch 99/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 362ms/step - accuracy: 0.9957 - loss: 0.0135 - val_accuracy: 0.9872 - val_loss: 0.0517\n",
      "Epoch 100/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 356ms/step - accuracy: 0.9956 - loss: 0.0103 - val_accuracy: 0.9973 - val_loss: 0.0094\n",
      "Epoch 101/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 357ms/step - accuracy: 0.9961 - loss: 0.0112 - val_accuracy: 0.9987 - val_loss: 0.0024\n",
      "Epoch 102/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 357ms/step - accuracy: 0.9984 - loss: 0.0047 - val_accuracy: 0.9980 - val_loss: 0.0055\n",
      "Epoch 103/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 355ms/step - accuracy: 0.9995 - loss: 0.0023 - val_accuracy: 0.9993 - val_loss: 0.0016\n",
      "Epoch 104/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 350ms/step - accuracy: 0.9979 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 0.0017\n",
      "Epoch 105/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 357ms/step - accuracy: 0.9988 - loss: 0.0042 - val_accuracy: 0.9987 - val_loss: 0.0091\n",
      "Epoch 106/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 358ms/step - accuracy: 0.9982 - loss: 0.0037 - val_accuracy: 0.9987 - val_loss: 0.0029\n",
      "Epoch 107/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 367ms/step - accuracy: 0.9986 - loss: 0.0031 - val_accuracy: 0.9993 - val_loss: 0.0023\n",
      "Epoch 108/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 352ms/step - accuracy: 0.9995 - loss: 0.0024 - val_accuracy: 0.9939 - val_loss: 0.0238\n",
      "Epoch 109/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 359ms/step - accuracy: 0.9974 - loss: 0.0050 - val_accuracy: 0.9987 - val_loss: 0.0025\n",
      "Epoch 110/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 357ms/step - accuracy: 0.9990 - loss: 0.0030 - val_accuracy: 0.9980 - val_loss: 0.0054\n",
      "Epoch 111/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 362ms/step - accuracy: 0.9984 - loss: 0.0044 - val_accuracy: 0.9973 - val_loss: 0.0051\n",
      "Epoch 112/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 365ms/step - accuracy: 0.9974 - loss: 0.0072 - val_accuracy: 0.9960 - val_loss: 0.0137\n",
      "Epoch 113/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 359ms/step - accuracy: 0.9978 - loss: 0.0039 - val_accuracy: 0.9960 - val_loss: 0.0065\n",
      "Epoch 114/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 361ms/step - accuracy: 0.9981 - loss: 0.0086 - val_accuracy: 0.9912 - val_loss: 0.0289\n",
      "Epoch 115/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 358ms/step - accuracy: 0.9994 - loss: 0.0023 - val_accuracy: 0.9966 - val_loss: 0.0080\n",
      "Epoch 116/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 350ms/step - accuracy: 0.9985 - loss: 0.0050 - val_accuracy: 0.9980 - val_loss: 0.0062\n",
      "Epoch 117/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 360ms/step - accuracy: 0.9977 - loss: 0.0057 - val_accuracy: 0.9966 - val_loss: 0.0059\n",
      "Epoch 118/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 354ms/step - accuracy: 0.9984 - loss: 0.0056 - val_accuracy: 0.9953 - val_loss: 0.0087\n",
      "Epoch 119/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 361ms/step - accuracy: 0.9990 - loss: 0.0035 - val_accuracy: 0.9987 - val_loss: 0.0079\n",
      "Epoch 120/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 350ms/step - accuracy: 0.9985 - loss: 0.0074 - val_accuracy: 0.9987 - val_loss: 0.0076\n",
      "Epoch 121/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 361ms/step - accuracy: 0.9975 - loss: 0.0090 - val_accuracy: 0.9987 - val_loss: 0.0016\n",
      "Epoch 122/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 351ms/step - accuracy: 0.9983 - loss: 0.0045 - val_accuracy: 0.9987 - val_loss: 0.0026\n",
      "Epoch 123/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 347ms/step - accuracy: 0.9986 - loss: 0.0032 - val_accuracy: 0.9987 - val_loss: 0.0072\n",
      "Epoch 124/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 352ms/step - accuracy: 0.9968 - loss: 0.0081 - val_accuracy: 0.9966 - val_loss: 0.0089\n",
      "Epoch 125/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 354ms/step - accuracy: 0.9978 - loss: 0.0039 - val_accuracy: 0.9865 - val_loss: 0.0559\n",
      "Epoch 126/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 366ms/step - accuracy: 0.9977 - loss: 0.0051 - val_accuracy: 0.9993 - val_loss: 0.0034\n",
      "Epoch 127/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 353ms/step - accuracy: 0.9996 - loss: 0.0017 - val_accuracy: 0.9973 - val_loss: 0.0036\n",
      "Epoch 128/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 356ms/step - accuracy: 0.9989 - loss: 0.0053 - val_accuracy: 0.9987 - val_loss: 0.0033\n",
      "Epoch 129/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 365ms/step - accuracy: 0.9983 - loss: 0.0050 - val_accuracy: 0.9987 - val_loss: 0.0044\n",
      "Epoch 130/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 354ms/step - accuracy: 0.9985 - loss: 0.0034 - val_accuracy: 0.9987 - val_loss: 0.0049\n",
      "Epoch 131/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 357ms/step - accuracy: 0.9981 - loss: 0.0064 - val_accuracy: 0.9966 - val_loss: 0.0111\n",
      "Epoch 132/250\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 352ms/step - accuracy: 0.9990 - loss: 0.0035 - val_accuracy: 0.9993 - val_loss: 0.0012\n",
      "\u001b[1m23/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.9997 - loss: 8.7725e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 02:03:11.477455: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1719', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 352ms/step - accuracy: 0.9996 - loss: 0.0011    \n",
      "Test Accuracy: 0.9986486434936523, Test Loss: 0.0035873239394277334\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Set paths to your dataset folders\n",
    "train_dir = '/home/esidserver/datasets/testing_cls/train/'\n",
    "val_dir = '/home/esidserver/datasets/testing_cls/val/'\n",
    "test_dir = '/home/esidserver/datasets/testing_cls/test/'\n",
    "\n",
    "# Image data generator with data augmentation for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Data generator for validation and test\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Data loaders\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_test_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = val_test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Load the ResNet-50 model with pre-trained weights\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Add custom top layers for classification\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze the base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping and model checkpoint\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('resnet50_best_model.keras', save_best_only=True, monitor='val_loss')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=250,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n",
    "\n",
    "# Fine-tuning: Unfreeze some layers of the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Compile the model again\n",
    "model.compile(optimizer=Adam(learning_rate=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Continue training with fine-tuning\n",
    "history_finetune = model.fit(\n",
    "    train_generator,\n",
    "    epochs=250,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {test_acc}, Test Loss: {test_loss}\")\n",
    "\n",
    "# Save the final model\n",
    "model.save('resnet50_final_model.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5193 images belonging to 4 classes.\n",
      "Found 1484 images belonging to 4 classes.\n",
      "Found 740 images belonging to 4 classes.\n",
      "Epoch 1/5\n",
      "\u001b[1m 43/163\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 375ms/step - accuracy: 0.2408 - loss: 1.4217"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 14:20:15.573553: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_5694', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m162/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 417ms/step - accuracy: 0.2704 - loss: 1.3937"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 14:21:16.763369: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1719', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 205ms/step\n",
      "   Epoch Validation Accuracy Precision  Recall  mAP@50 mAP@50-95\n",
      "0      1              0.4420    0.4645  0.2729  0.1917    0.1917\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 610ms/step - accuracy: 0.2709 - loss: 1.3934 - val_accuracy: 0.4420 - val_loss: 1.3176\n",
      "Epoch 2/5\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 99ms/step\n",
      "   Epoch Validation Accuracy Precision  Recall  mAP@50 mAP@50-95\n",
      "0      2              0.3666    0.2835  0.2588  0.1904    0.1904\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 448ms/step - accuracy: 0.3656 - loss: 1.3320 - val_accuracy: 0.3666 - val_loss: 1.2784\n",
      "Epoch 3/5\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 104ms/step\n",
      "   Epoch Validation Accuracy Precision  Recall  mAP@50 mAP@50-95\n",
      "0      3              0.4508    0.2077  0.2332  0.1764    0.1764\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 445ms/step - accuracy: 0.4047 - loss: 1.2856 - val_accuracy: 0.4508 - val_loss: 1.2463\n",
      "Epoch 4/5\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 90ms/step\n",
      "   Epoch Validation Accuracy Precision  Recall  mAP@50 mAP@50-95\n",
      "0      4              0.4966    0.1706  0.2284  0.1948    0.1948\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 419ms/step - accuracy: 0.4384 - loss: 1.2509 - val_accuracy: 0.4966 - val_loss: 1.1981\n",
      "Epoch 5/5\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 100ms/step\n",
      "   Epoch Validation Accuracy Precision  Recall  mAP@50 mAP@50-95\n",
      "0      5              0.4960    0.1880  0.2480  0.2062    0.2062\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 419ms/step - accuracy: 0.4567 - loss: 1.2218 - val_accuracy: 0.4960 - val_loss: 1.2034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/esidserver/miniforge3/envs/tf/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 14:27:10.465755: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_15690', 76 bytes spill stores, 76 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 190ms/step\n",
      "   Epoch Validation Accuracy Precision  Recall  mAP@50 mAP@50-95\n",
      "0      1              0.2635    0.2224  0.2507  0.1136    0.1136\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 753ms/step - accuracy: 0.4565 - loss: 2.5147 - val_accuracy: 0.2635 - val_loss: 1.7609\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/esidserver/miniforge3/envs/tf/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 105ms/step\n",
      "   Epoch Validation Accuracy Precision  Recall  mAP@50 mAP@50-95\n",
      "0      2              0.2493    0.0617  0.2473  0.0987    0.0987\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 430ms/step - accuracy: 0.8470 - loss: 0.3905 - val_accuracy: 0.2493 - val_loss: 2.6635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/esidserver/miniforge3/envs/tf/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 1/24\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 176ms/step - accuracy: 0.0625 - loss: 1.2513"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/esidserver/miniforge3/envs/tf/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.1371 - loss: 1.6864"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 14:31:00.248505: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1719', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 293ms/step - accuracy: 0.1464 - loss: 1.6936\n",
      "Test Accuracy: 0.2541, Test Loss: 1.7766\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 201ms/step\n",
      "Confusion Matrix:\n",
      "[[  4   0 183   1]\n",
      " [  5   0 179   0]\n",
      " [  0   0 184   0]\n",
      " [  9   0 175   0]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set paths to your dataset folders\n",
    "train_dir = '/home/esidserver/datasets/testing_cls/train/'\n",
    "val_dir = '/home/esidserver/datasets/testing_cls/val/'\n",
    "test_dir = '/home/esidserver/datasets/testing_cls/test/'\n",
    "\n",
    "# Image data generator with data augmentation for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Data generator for validation and test\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Data loaders\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_test_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = val_test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  # Important for confusion matrix\n",
    ")\n",
    "\n",
    "# Load the ResNet-50 model with pre-trained weights\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Add custom top layers for classification\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze the base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define a custom callback to display live metrics\n",
    "class LiveMetrics(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Calculate precision, recall, f1-score using validation data\n",
    "        val_preds = self.model.predict(val_generator)\n",
    "        val_pred_classes = np.argmax(val_preds, axis=1)\n",
    "        val_true_classes = val_generator.classes\n",
    "\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(val_true_classes, val_pred_classes, average='weighted')\n",
    "\n",
    "        # Using F1-score as an approximation for mAP@50 and mAP@50-95\n",
    "        map50 = f1\n",
    "        map50_95 = f1\n",
    "\n",
    "        # Print live metrics table\n",
    "        results = {\n",
    "            \"Epoch\": epoch + 1,\n",
    "            \"Validation Accuracy\": f\"{logs['val_accuracy']:.4f}\",\n",
    "            \"Precision\": f\"{precision:.4f}\",\n",
    "            \"Recall\": f\"{recall:.4f}\",\n",
    "            \"mAP@50\": f\"{map50:.4f}\",\n",
    "            \"mAP@50-95\": f\"{map50_95:.4f}\"\n",
    "        }\n",
    "        results_df = pd.DataFrame([results])\n",
    "        print(results_df)\n",
    "\n",
    "# Define early stopping and model checkpoint\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('resnet50_best_model.keras', save_best_only=True, monitor='val_loss')\n",
    "live_metrics = LiveMetrics()\n",
    "\n",
    "# Train the model with live metrics callback\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=5,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[early_stopping, model_checkpoint, live_metrics]\n",
    ")\n",
    "\n",
    "# Fine-tuning: Unfreeze some layers of the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Compile the model again\n",
    "model.compile(optimizer=Adam(learning_rate=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Continue training with fine-tuning and live metrics callback\n",
    "history_finetune = model.fit(\n",
    "    train_generator,\n",
    "    epochs=2,  # Fine-tuning for 100 epochs\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[early_stopping, model_checkpoint, live_metrics]\n",
    ")\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Save the final model\n",
    "model.save('resnet50_final_model.keras')\n",
    "\n",
    "# Generate predictions for the test set\n",
    "y_pred = model.predict(test_generator)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Get true labels\n",
    "y_true = test_generator.classes\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred_classes)\n",
    "cm_labels = list(test_generator.class_indices.keys())\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(f\"Confusion Matrix:\\n{cm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5193 images belonging to 4 classes.\n",
      "Found 1484 images belonging to 4 classes.\n",
      "Found 740 images belonging to 4 classes.\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/esidserver/miniforge3/envs/tf/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 472ms/step - accuracy: 0.2730 - loss: 1.4188 - val_accuracy: 0.2817 - val_loss: 1.3258\n",
      "Epoch 2/5\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 402ms/step - accuracy: 0.3474 - loss: 1.3393 - val_accuracy: 0.3349 - val_loss: 1.3055\n",
      "Epoch 3/5\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 420ms/step - accuracy: 0.4005 - loss: 1.3018 - val_accuracy: 0.4178 - val_loss: 1.2517\n",
      "Epoch 4/5\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 410ms/step - accuracy: 0.4301 - loss: 1.2635 - val_accuracy: 0.4427 - val_loss: 1.2288\n",
      "Epoch 5/5\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 413ms/step - accuracy: 0.4188 - loss: 1.2414 - val_accuracy: 0.4764 - val_loss: 1.1763\n",
      "Epoch 1/2\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 651ms/step - accuracy: 0.4642 - loss: 2.5812 - val_accuracy: 0.2554 - val_loss: 2.7156\n",
      "Epoch 2/2\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 410ms/step - accuracy: 0.8412 - loss: 0.4134 - val_accuracy: 0.2729 - val_loss: 1.9116\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 101ms/step - accuracy: 0.2562 - loss: 2.7035\n",
      "Final Validation Accuracy: 0.2554, Validation Loss: 2.7156\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 220ms/step - accuracy: 0.5613 - loss: 1.5557\n",
      "Test Accuracy: 0.2541, Test Loss: 2.7085\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 257ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AB       0.25      1.00      0.41       188\n",
      "          AG       0.00      0.00      0.00       184\n",
      "         ANO       0.00      0.00      0.00       184\n",
      "          CQ       0.00      0.00      0.00       184\n",
      "\n",
      "    accuracy                           0.25       740\n",
      "   macro avg       0.06      0.25      0.10       740\n",
      "weighted avg       0.06      0.25      0.10       740\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/esidserver/miniforge3/envs/tf/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/esidserver/miniforge3/envs/tf/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/esidserver/miniforge3/envs/tf/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set paths to your dataset folders\n",
    "train_dir = '/home/esidserver/datasets/testing_cls/train/'\n",
    "val_dir = '/home/esidserver/datasets/testing_cls/val/'\n",
    "test_dir = '/home/esidserver/datasets/testing_cls/test/'\n",
    "\n",
    "# Image data generator with data augmentation for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Data generator for validation and test\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Data loaders\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_test_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = val_test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  # Important for confusion matrix\n",
    ")\n",
    "\n",
    "# Load the ResNet-50 model with pre-trained weights\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Add custom top layers for classification\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze the base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping and model checkpoint\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('resnet50_best_model.keras', save_best_only=True, monitor='val_loss')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=5,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n",
    "\n",
    "# Fine-tuning: Unfreeze some layers of the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Compile the model again\n",
    "model.compile(optimizer=Adam(learning_rate=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Continue training with fine-tuning\n",
    "history_finetune = model.fit(\n",
    "    train_generator,\n",
    "    epochs=2,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n",
    "\n",
    "# Evaluate on the validation set\n",
    "val_loss, val_acc = model.evaluate(val_generator)\n",
    "print(f\"Final Validation Accuracy: {val_acc:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Save the final model\n",
    "model.save('resnet50_final_model.keras')\n",
    "\n",
    "# Generate predictions for the test set\n",
    "y_pred = model.predict(test_generator)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Get true labels\n",
    "y_true = test_generator.classes\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred_classes)\n",
    "cm_labels = list(test_generator.class_indices.keys())\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=cm_labels, yticklabels=cm_labels)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report for precision, recall, f1-score\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred_classes, target_names=cm_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
